{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a103ff9",
   "metadata": {},
   "source": [
    "### 1️⃣ 데이터 준비 및 전처리\n",
    "\n",
    "#### 카테고리 선택 및 라벨 재정렬\n",
    "```python\n",
    "def filter_categories(dataset, categories):\n",
    "    target_names = dataset.target_names\n",
    "    selected_idx = [target_names.index(c) for c in categories]\n",
    "    data_filtered, target_filtered = [], []\n",
    "    for text, label in zip(dataset.data, dataset.target):\n",
    "        if label in selected_idx:\n",
    "            new_label = selected_idx.index(label)  # 라벨 재정렬\n",
    "            data_filtered.append(text)\n",
    "            target_filtered.append(new_label)\n",
    "    return data_filtered, target_filtered, categories\n",
    "\n",
    "train_data, train_target, target_names = filter_categories(newsgroups_train, categories)\n",
    "test_data, test_target, _ = filter_categories(newsgroups_test, categories)\n",
    "```\n",
    "\n",
    "#### 텍스트 정제 (헤더, 풋터, 인용문 제거)\n",
    "```python\n",
    "import re\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'^From:.*\\n', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'^Subject:.*\\n', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\n--\\n.*$', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'(^|\\n)[>|:].*', '', text)\n",
    "    return text\n",
    "\n",
    "train_data = [clean_text(t) for t in train_data]\n",
    "test_data = [clean_text(t) for t in test_data]\n",
    "\n",
    "len(train_data), len(train_target), len(test_data), len(test_target)\n",
    "```\n",
    "\n",
    "### 2️⃣ 벡터화 및 기초 모델\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "```\n",
    "\n",
    "#### BOW 벡터화\n",
    "```python\n",
    "cv = CountVectorizer(max_features=2000, min_df=5, max_df=0.5)\n",
    "x_train_cv = cv.fit_transform(train_data)\n",
    "x_test_cv = cv.transform(test_data)\n",
    "```\n",
    "#### Multinomial Naive Bayes 학습\n",
    "```python\n",
    "nb = MultinomialNB()\n",
    "nb.fit(x_train_cv, train_target)\n",
    "print(\"BOW + MNB:\", nb.score(x_train_cv, train_target), nb.score(x_test_cv, test_target))\n",
    "```\n",
    "#### TF-IDF 벡터화\n",
    "```python\n",
    "tfidf = TfidfVectorizer(max_features=2000, min_df=5, max_df=0.5)\n",
    "x_train_tfidf = tfidf.fit_transform(train_data)\n",
    "x_test_tfidf = tfidf.transform(test_data)\n",
    "```\n",
    "#### TF-IDF + MNB\n",
    "```python\n",
    "nb_tfidf = MultinomialNB()\n",
    "nb_tfidf.fit(x_train_tfidf, train_target)\n",
    "print(\"TF-IDF + MNB:\", nb_tfidf.score(x_train_tfidf, train_target), nb_tfidf.score(x_test_tfidf, test_target))\n",
    "```\n",
    "\n",
    "### 3️⃣ Logistic Regression 및 규제\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train_tfidf, train_target, test_size=0.2, stratify=train_target, random_state=42\n",
    ")\n",
    "```\n",
    "#### 일반 Logistic Regression\n",
    "```python\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(x_train_tfidf, train_target)\n",
    "print(\"LogisticRegression:\", lr.score(x_train_tfidf, train_target), lr.score(x_test_tfidf, test_target))\n",
    "```\n",
    "#### RidgeClassifier (L2 규제)\n",
    "```python\n",
    "rc = RidgeClassifier(alpha=15)\n",
    "rc.fit(x_train, y_train)\n",
    "print(\"RidgeClassifier:\", rc.score(x_train, y_train), rc.score(x_val, y_val))\n",
    "```\n",
    "#### L1 규제 Logistic (특성 선택)\n",
    "```python\n",
    "l1_lr = LogisticRegression(penalty='l1', max_iter=1000, solver='l\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975a2916",
   "metadata": {},
   "source": [
    "### 분류 모델 요약\n",
    "\n",
    "#### 1. RidgeClassifier\n",
    "- **설명**: 선형 모델에 L2 규제를 적용한 분류기. 과적합을 완화하고 가중치를 안정적으로 학습.\n",
    "- **특징**:\n",
    "  - 희소(sparse) 데이터와 잘 맞음\n",
    "  - 학습 속도가 빠름\n",
    "  - 규제(alpha)로 과적합 조절 가능\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. DecisionTreeClassifier\n",
    "- **설명**: 데이터를 기준(feature)으로 나누어 트리 구조로 분류하는 모델.\n",
    "- **특징**:\n",
    "  - 직관적인 규칙 기반 분류\n",
    "  - 과적합 위험이 있음 (특히 깊은 트리)\n",
    "  - 시각화가 쉬움\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. RandomForestClassifier\n",
    "- **설명**: 여러 개의 Decision Tree를 만들어 투표 방식으로 예측하는 앙상블 모델.\n",
    "- **특징**:\n",
    "  - 과적합 방지 및 일반화 성능 향상\n",
    "  - 변수 중요도(feature importance) 제공\n",
    "  - 안정적이고 강력한 성능\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. GradientBoostingClassifier\n",
    "- **설명**: 이전 트리의 오차를 보정하면서 순차적으로 트리를 학습하는 부스팅 기반 앙상블.\n",
    "- **특징**:\n",
    "  - 강력한 예측 성능\n",
    "  - 학습 속도 느릴 수 있음\n",
    "  - 과적합 방지를 위해 learning_rate, n_estimators 조절 필요"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
