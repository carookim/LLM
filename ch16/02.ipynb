{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Opo3e83NAehQ"
      },
      "outputs": [],
      "source": [
        "# Train api\n",
        "# pytorch 직접 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Zj020VtOA9Lt"
      },
      "outputs": [],
      "source": [
        "# 전처리\n",
        "from transformers import BertTokenizer\n",
        "BERT_NAME = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(BERT_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "5lvrh3ivBy4H"
      },
      "outputs": [],
      "source": [
        "# 데이터 준비\n",
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RweQuBNjCgJ-",
        "outputId": "7c9ca325-12a2-4ba1-f139-c14acd592d56"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('movie_reviews')\n",
        "ids = movie_reviews.fileids()\n",
        "reviews = [movie_reviews.raw(id) for id in ids]\n",
        "categories = [movie_reviews.categories(id)[0] for id in ids]\n",
        "# 값이 하나만 있는 categoried에 왜\n",
        "# 인덱스 0을 입력해서 값을 가져오는 이유 :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "b_ezoEfFDr2W"
      },
      "outputs": [],
      "source": [
        "# 라벨인코딩\n",
        "y = [1 if c=='pos' else 0 for c in categories]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "WzRjXHHxDzou"
      },
      "outputs": [],
      "source": [
        "# train/test split\n",
        "x_train,x_test,y_train,y_test =  train_test_split(reviews,y,random_state=42,test_size=0.2,shuffle=True,stratify=y) #^\n",
        "# 여기서 굳이 stracify=y가 필요한지 ^"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Rg5AZCoCEIDn"
      },
      "outputs": [],
      "source": [
        "# 데이터 셋\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class MovieReviewDatasets(Dataset):\n",
        "  '''\n",
        "    Args\n",
        "        encodings : 토크나이져된 값(딕셔너리형태)\n",
        "        labels : 라벨링된 클래스값(0 1)\n",
        "  '''\n",
        "  def __init__(self,encodings,labels):\n",
        "    self.encodings = encodings\n",
        "    self.labels = labels\n",
        "  def __getitem__(self,index): # torch 데이터를 받는다\n",
        "    item = {\n",
        "        key:val[index].clone().detach()\n",
        "        for key,val in self.encodings.items()\n",
        "    }\n",
        "    item['labels'] = torch.tensor(self.labels[index],dtype=torch.long)\n",
        "    return item\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "# 토큰화 (BertTOkenizerFast) -> huggingFace 공식 추천\n",
        "from transformers import BertTokenizerFast\n",
        "# 토크나이저 생성\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "train_encodings = tokenizer(\n",
        "    x_train,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=512,\n",
        "    return_tensors='pt' #^\n",
        ")\n",
        "test_encodings = tokenizer(\n",
        "    x_test,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=512,\n",
        "    return_tensors='pt' #^\n",
        ")\n",
        "train_dataset = MovieReviewDatasets(train_encodings,y_train)\n",
        "test_dataset = MovieReviewDatasets(test_encodings,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "AETEHkszYIJ5"
      },
      "outputs": [],
      "source": [
        "# | 값      | 의미                                      |\n",
        "# | ------ | --------------------------------------- |\n",
        "# | `'pt'` | PyTorch **tensor**로 반환 (`torch.Tensor`) |\n",
        "# | `'tf'` | TensorFlow **tensor**로 반환 (`tf.Tensor`) |\n",
        "# | `'np'` | NumPy 배열로 반환 (`np.ndarray`)             |\n",
        "# | `None` | 기본값, 그냥 **Python 리스트/딕셔너리**로 반환         |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zf_Nhi6_Ymyr",
        "outputId": "6016392b-96c5-4d74-de78-3522382f25c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(next(iter(train_dataset))['input_ids'])\n",
        "# iter(X)\n",
        "# 반복 가능한 객체(iterable)를 반복자(iterator)로 변환해주는 함수\n",
        "# X를 반복자로 변환하는 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "LpuN8Sgmfb7K"
      },
      "outputs": [],
      "source": [
        "# %pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "uDoQyUpqZJ7B",
        "outputId": "ecd6e204-2833-4bed-bcb3-32bc88eb18ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='288' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [288/600 04:17 < 04:40, 1.11 it/s, Epoch 1.44/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.475800</td>\n",
              "      <td>0.363128</td>\n",
              "      <td>0.857500</td>\n",
              "      <td>0.928144</td>\n",
              "      <td>0.775000</td>\n",
              "      <td>0.844687</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Trainer API\n",
        "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import evaluate  # load_metric\n",
        "# 모델 로드  클래스개수 2\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)\n",
        "accuracy_metric = evaluate.load('accuracy')\n",
        "precision_metric = evaluate.load('precision')\n",
        "recall_metric = evaluate.load('recall')\n",
        "f1_metric = evaluate.load(\"f1\", average='weighted') #^\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        'accuracy' : accuracy_metric.compute(predictions=predictions, references=labels)['accuracy'],\n",
        "        'precision' : precision_metric.compute(predictions=predictions, references=labels)['precision'],\n",
        "        'recall' : recall_metric.compute(predictions=predictions, references=labels)['recall'],\n",
        "        'f1' : f1_metric.compute(predictions=predictions, references=labels)['f1'],\n",
        "    }\n",
        "\n",
        "# TraingAguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,             # NLP 에서는 2 ~5\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=16, # 평가는 gradient 안하므로 train보다 크게설정하는 경향\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,  # L2정규화 규제 강도\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='f1',\n",
        "    report_to='none'\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = test_dataset,\n",
        "    compute_metrics = compute_metrics\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PX60kuOj7nCV"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crNYR1y065Qv"
      },
      "outputs": [],
      "source": [
        "# Pytorch로 직접개선\n",
        "# 메모리 정리\n",
        "del model, trainer\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# BertForSequenceClassification --> Bert 전용 모델\n",
        "# 분류용 헤드를 포함 : [CLS]토큰을 출력 -> Linear layer -> logits\n",
        "\n",
        "# BertModel : BERT 전용\n",
        "# 분류헤드 없음  -> 분류 회귀 QA등 작업에 바로쓰려면 헤드를 붙여야 한다.\n",
        "from transformers import BertModel\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "import time\n",
        "\n",
        "# Bert 모델\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "class impreovedBertClassifier(torch.nn.Module):\n",
        "  def __init__(self, hidden_size):\n",
        "    super(ImprovedBerClassifier, self).__init__()\n",
        "    '''\n",
        "    개선버젼\n",
        "    - Drop out 추가\n",
        "    - 초기화 개선\n",
        "    '''\n",
        "    def __init__(self,pretrained_model,num_labels=2,dropout=0.1):\n",
        "        super(ImprovedBertClassifier, self).__init__()\n",
        "        self.bert = pretrained_model\n",
        "        self.drop_out = torch.nn.Dropout(dropout)\n",
        "        self.classifier = torch.nn.Linear(self.bert.config.hidden_size,num_labels)\n",
        "        # BERT 분류기 가중치 초기화 - 선형계층 초기화(입력과 출력의 분산을 일정하게 유지)\n",
        "        torch.nn.init.xavier_uniform_(self.classifier.weight)\n",
        "        # 편향을 초기화\n",
        "        torch.nn.init.zeros_(self.classifier.bias)\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "      self.bert(\n",
        "          input_ids=input_ids,\n",
        "          attention_mask=attention_mask,\n",
        "          token_type_ids=token_type_ids\n",
        "      )\n",
        "      cls_output = outputs.last_hidden_state[:,0,:]\n",
        "      cls_output = self.dropout(cls_output)\n",
        "      return self.classifier(cls_output)\n",
        "# 모델 초기화\n",
        "model = ImprovedBertClassifier(bert_model,num_labels=2)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "\n",
        "# dataloader\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "# 옵티마이저 손실함수\n",
        "optimizer = AdamW(model.parameters(),lr=2e-5,weight_decay=0.01)\n",
        "# crossentropyloss는 softmax가 적용\n",
        "criteriion = torch.nn.CrossEntropyLoss()\n",
        "num_epochs = 3\n",
        "start = time.time()\n",
        "# 학습루프\n",
        "for epoch in range(num_epochs):\n",
        "  total_loss = 0\n",
        "  for step.batch in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    # 입력준비\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention = batch['attention_mask'].to(device)\n",
        "    token_type_ids = batch['token_type_ids'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "    # forward\n",
        "    output = model(input_ids, attention_mask, token_type_ids)\n",
        "    loss = criterion(ouputs,labels)\n",
        "\n",
        "    # backward\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "  epoch_loss = total_loss / len(train_loader)\n",
        "  print(f'epoch : {epoch+1} loss : {epcoh_loss}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "conda_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
