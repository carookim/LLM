{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d682539a",
   "metadata": {},
   "source": [
    "### ìì—°ì–´ ê°ì„± ë¶„ì„\n",
    "- ê°ì„±ì‚¬ì „ ê¸°ë°˜ : ë¯¸ë¦¬ ì €ìœ¼ì´ëœ ê°ì„±ë‹¨ì–´ ì‚¬ì „ ì‚¬ìš©(ê·œì¹™ ê¸°ë°˜)\n",
    "    - TextBlob, AFINNm VADER\n",
    "- ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ : ë°ì´í„°ë¡œ ë¶€í„° íŒ¨í„´ í•™ìŠµ(í†µê³„ ê¸°ë°˜)\n",
    "    - TF-IDF ë²¡í„°í™”\n",
    "    - ë¹ˆë„ìˆ˜ ë²¡í„°í™”\n",
    "    - Multinomial Naive Bayes\n",
    "    - ë¡œì§€ìŠ¤í‹± íšŒê·€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ad631e",
   "metadata": {},
   "source": [
    "### ì‚¬ìš© ë°ì´í„°\n",
    "- NLTK ì˜í™” ë¦¬ë·° (2000ê°œ)\n",
    "- ë‹¤ìŒì˜í™”ë¦¬ë·°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28579dc1",
   "metadata": {},
   "source": [
    "#### ì•Œê³ ë¦¬ì¦˜\n",
    "- textblob\n",
    "- afinn\n",
    "- vader\n",
    "\n",
    "- TF-IDF\n",
    "- CounterVectorizer\n",
    "- Multinomial Naive Bayes\n",
    "- ë¡œì§€ìŠ¤í‹± íšŒê·€ ^"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3778df5a",
   "metadata": {},
   "source": [
    "### TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0851ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextBlob\n",
    "# íŠ¹ì •ë‹¨ì–´ë¥¼ ì„ ì •í•˜ê³ , ê·¸ ë‹¨ì–´ê°€ ë‚˜ì˜¬ê²½ìš° ì ìˆ˜ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ê°ì í•œë‹¤.\n",
    "# Polarity  ê·¹ì„±ë„\n",
    "# ê¸ì •(1)ê³¼ ë¶€ì •(-1)\n",
    "# 0ì€ ì¤‘ë¦½\n",
    "# Subjectivity ì£¼ê´€ì„±\n",
    "# ê°ê´€(-1)ê³¼ ì£¼ê´€(1)\n",
    "# 0ì€ ì¤‘ë¦½\n",
    "\n",
    "# ë¬¸ë§¥ì€ ë¬´ì‹œí•˜ê³  ë‹¨ì–´ ì—¬ë¶€ë¡œë§Œ íŒë‹¨í•œë‹¤.\n",
    "# ì´ ì˜í™”ëŠ” ë‚˜ì˜ì§€ ì•Šë‹¤. -> ë‚˜ì˜ë‹¤, ì•Šë‹¤ ì—ì„œ ê°ì (-)ìœ¼ë¡œ ì¸ì‹\n",
    "# ì¥ì  : ë¹ ë¥¸ì†ë„ í•™ìŠµ ë¶ˆí•„ìš”\n",
    "# ì‚¬ìš© : ì‹¤ì‹œê°„ ê°ì„± ë¶„ì„, ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° -> ë¹ ë¥¸ íŒë‹¨ì´ í•„ìš”í•œ ìƒí™©ì— ì‚¬ìš©í•˜ê¸°ë„ í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be1837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d202b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob, Word\n",
    "text = \"TextBlob is amazingly simple to use. What a wonderful library for NLP!\"\n",
    "blob = TextBlob(text)\n",
    "print(blob.sentences)\n",
    "print(blob.words)\n",
    "print(blob.tags)\n",
    "\n",
    "import nltk\n",
    "# nltk.download('brown')\n",
    "print(blob.noun_phrases)\n",
    "\n",
    "# ë¬¸ì¥, ë‹¨ì–´, í’ˆì‚¬ ë‹¨ìœ„ì˜ ê¸°ë³¸ ë¶„ì„ ê¸°ëŠ¥ì„ ì§€ì›í•¨ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.\n",
    "# ë‹¨ì–´ ë¶„ë¦¬ì˜ ê²½ìš°ì—ëŠ” ë¬´ì—‡ì„ ê¸°ì¤€ìœ¼ë¡œ? ^ ê³µë°±, ë¬¸ì¥ë¶€í˜¸, í˜•íƒœì†Œ ë¶„ì„ ê·œì¹™ì„ ê¸°ì¤€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4512b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°ì„± ë¶„ì„\n",
    "blob.sentiment\n",
    "# Sentiment(polarity=0.5, subjectivity=0.6785714285714286)\n",
    "\n",
    "# Polarity  ê·¹ì„±ë„\n",
    "# ê¸ì •(1)ê³¼ ë¶€ì •(-1)\n",
    "# 0ì€ ì¤‘ë¦½\n",
    "# Subjectivity ì£¼ê´€ì„±\n",
    "# ê°ê´€(-1)ê³¼ ì£¼ê´€(1)\n",
    "# 0ì€ ì¤‘ë¦½\n",
    "\n",
    "# ì•½ê°„ ê¸ì •ì´ê³ , ì£¼ê´€ì ì¸ í‘œí˜„ì´ë‹¤.\n",
    "\n",
    "print(f'polarity : {blob.sentiment.polarity}')\n",
    "print(f'subjectivity : {blob.sentiment.subjectivity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b56daf",
   "metadata": {},
   "source": [
    "### AFINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff87ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFINN(Lexicon-Based)\n",
    "# ê° ë‹¨ì–´ì˜ -5 ~ +5ì˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ê³  í•©ì‚°\n",
    "# ì´ ì˜í™”ëŠ” ì¢‹ì§€ë§Œ ì¢‹ì§€ì•Šì€ ë¶€ë¶„ë„ ìˆë‹¤.\n",
    "    # ì¢‹ë‹¤ +3 ì¢‹ë‹¤ +3 ì•Šë‹¤ -3 = +3 > 0 -> ê¸ì • ìœ¼ë¡œ í‰ê°€\n",
    "# ì¡°íšŒ ë°©ë²• : score = sum(word_sentiment_value)\n",
    "# ë¶„ë¥˜ê·œì¹™\n",
    "    # 0 > score ê¸ì •\n",
    "    # score < 0 ë¶€ì •\n",
    "# ì´ëª¨í‹°ì½˜ì§€ì›\n",
    "# ê°•ë„í‘œí˜„ ì¸ì‹ very, really ë“±\n",
    "\n",
    "# ê°•ì¡° ìˆ˜ì •ì(intensifiers)\n",
    "    # ë§¤ìš°ì¢‹ë‹¤ = 1.5 x (ì¢‹ë‹¤ì˜ ì ìˆ˜)\n",
    "\n",
    "# AFINN vs TextBlob\n",
    "# AFINN : ë” ì •í™•í•œ ì ìˆ˜ ë§¤í•‘\n",
    "# TextBlob : ë” ì¼ë°˜ì ì¸ ì ‘ê·¼ ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5220e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting afinn\n",
      "  Downloading afinn-0.1.tar.gz (52 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: afinn\n",
      "  Building wheel for afinn (setup.py): started\n",
      "  Building wheel for afinn (setup.py): finished with status 'done'\n",
      "  Created wheel for afinn: filename=afinn-0.1-py3-none-any.whl size=53479 sha256=d5e78352c9a512aa4d0d73e7468f036fd2534ea239a3df9b2abd21038a65bdfb\n",
      "  Stored in directory: c:\\users\\playdata2\\appdata\\local\\pip\\cache\\wheels\\f3\\20\\88\\bd79bf1dfa529d0e4e301372371edf7639514a9f0d337769c3\n",
      "Successfully built afinn\n",
      "Installing collected packages: afinn\n",
      "Successfully installed afinn-0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'afinn' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'afinn'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n"
     ]
    }
   ],
   "source": [
    "# %pip install afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab61b0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 4.0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from afinn import Afinn\n",
    "af = Afinn()\n",
    "text1 = \"TextBlob is amazingly simple to use.\"\n",
    "text2 = \"What a wonderful library for NLP!\"\n",
    "score1 = af.score(text1)\n",
    "score2 = af.score(text2)\n",
    "score1, score2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a44711b",
   "metadata": {},
   "source": [
    "### VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6f2fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER ì†Œì…œë¯¸ë””ì–´ í…ìŠ¤íŠ¸ì— ìµœì í™”\n",
    "# ì´ ì˜í™”ëŠ” ì •ë§ì •ë§ í›Œë¥­í•´!!!\n",
    "# í›Œë¥­í•˜ë‹¤ (ê¸°ë³¸) + 0.7 ì •ë§ì •ë§ (ê°•ì¡°) x 1.5\n",
    "# !!! (ë¬¸ì¥ë¶€í˜¸ê°•ì¡°)  x 1.2\n",
    "\n",
    "# 4ê°œì˜ ê°ì • ì§€ìˆ˜\n",
    "    # positive ê¸ì • í™•ë¥  0 ~ 1\n",
    "    # negative ë¶€ì • í™•ë¥ \n",
    "    # neutral ì¤‘ë¦½ í™•ë¥ \n",
    "    # compund ì¢…í•©ì ìˆ˜ -1 ~ 1\n",
    "        # +1 ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë§¤ìš° ê¸ì •ì \n",
    "        # -1 ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë§¤ìš° ë¶€ì •ì \n",
    "        # 0 ê·¼ì²˜ë¼ë©´ ì¤‘ë¦½ì \n",
    "\n",
    "# ì¢…í•©ì ìˆ˜ ì¡°íšŒ ë°©ë²• :\n",
    "# score = compound_score / sqrt(compound_score**2 + 0.0625)\n",
    "# score >= 0.05 ê¸ì •\n",
    "# score <= -0.05 ë¶€ì •\n",
    "# ê·¸ ì‚¬ì´( -0.05 < score < 0.05 )ëŠ” ì¤‘ë¦½\n",
    "\n",
    "# ëŒ€ì†Œë¬¸ì êµ¬ë¶„ AMAZING amazing ë‹¤ë¥¸ ì ìˆ˜\n",
    "# :) ê¸ì • :-( ë¶€ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b047040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Playdata2\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d872e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¬¸ì¥ : I love this product! It's absolutely amazing ğŸ˜\n",
      "ì ìˆ˜ : {'neg': 0.0, 'neu': 0.318, 'pos': 0.682, 'compound': 0.862}\n",
      "ë¬¸ì¥ : This is the worst movie I've ever seen...\n",
      "ì ìˆ˜ : {'neg': 0.369, 'neu': 0.631, 'pos': 0.0, 'compound': -0.6249}\n",
      "ë¬¸ì¥ : The food was okay, not great but not bad either.\n",
      "ì ìˆ˜ : {'neg': 0.149, 'neu': 0.487, 'pos': 0.364, 'compound': 0.4728}\n",
      "ë¬¸ì¥ : Iâ€™m REALLY happy with the results!!!\n",
      "ì ìˆ˜ : {'neg': 0.0, 'neu': 0.472, 'pos': 0.528, 'compound': 0.7651}\n",
      "ë¬¸ì¥ : Not good at all. Iâ€™m disappointed.\n",
      "ì ìˆ˜ : {'neg': 0.579, 'neu': 0.421, 'pos': 0.0, 'compound': -0.6711}\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "sentences = [\n",
    "    \"I love this product! It's absolutely amazing ğŸ˜\",\n",
    "    \"This is the worst movie I've ever seen...\",\n",
    "    \"The food was okay, not great but not bad either.\",\n",
    "    \"Iâ€™m REALLY happy with the results!!!\",\n",
    "    \"Not good at all. Iâ€™m disappointed.\",\n",
    "]\n",
    "\n",
    "for s in sentences:\n",
    "    scores = analyzer.polarity_scores(s)\n",
    "    print(f'ë¬¸ì¥ : {s}')\n",
    "    print(f'ì ìˆ˜ : {scores}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5920e92",
   "metadata": {},
   "source": [
    "### ì¢…í•© ì •ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3ec6d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "import nltk\n",
    "from sklearn.metrics import accuracy_score\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# nltk ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
    "nltk.download('movie_reviews',quiet=True)\n",
    "nltk.download('vader_lexicon',quiet=True)\n",
    "\n",
    "# ì˜í™” ë¦¬ë·°ë°ì´í„° ë¡œë“œ\n",
    "fileids = movie_reviews.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42ad94fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 50, 50)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = [movie_reviews.raw(fileid) for fileid in fileids[:50]] + \\\n",
    "        [movie_reviews.raw(fileid) for fileid in fileids[-50:]]\n",
    "categories = [movie_reviews.categories(fileid)[0]for fileid in fileids[:50]] + \\\n",
    "        [movie_reviews.categories(fileid)[0]for fileid in fileids[-50:]]\n",
    "# + \\ ì´ê±° ë­”ì§€ ^\n",
    "\n",
    "# +\n",
    "# ë¦¬ìŠ¤íŠ¸ í•©ì¹˜ê¸°(concatenation) ì—­í• \n",
    "\n",
    "# \\\n",
    "# íŒŒì´ì¬ì—ì„œ ì¤„ë°”ê¿ˆ(line continuation) í‘œì‹œ\n",
    "# í•œ ì¤„ì´ ë„ˆë¬´ ê¸¸ ë•Œ, ë‹¤ìŒ ì¤„ë¡œ ì´ì–´ì„œ ì“¸ ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.\n",
    "\n",
    "len(reviews), categories.count('pos'), categories.count('neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb733d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì •í™•ë„ :0.6\n"
     ]
    }
   ],
   "source": [
    "# 1. TextBlob\n",
    "def sentiment_textblob(docs):\n",
    "    return ['pos' if TextBlob(doc).sentiment.polarity > 0 else 'neg' for doc in docs ]\n",
    "predictions_textblob = sentiment_textblob(reviews)\n",
    "accuracy_textblob = accuracy_score(categories,predictions_textblob)\n",
    "print(f'ì •í™•ë„ :{accuracy_textblob:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03eef22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì •í™•ë„ :0.7\n"
     ]
    }
   ],
   "source": [
    "# 2. AFINN\n",
    "def sentiment_afinn(docs):\n",
    "    afn = Afinn(emoticons=True)\n",
    "    return ['pos' if afn.score(doc) > 0 else 'neg' for doc in docs ]\n",
    "predictions_afinn = sentiment_afinn(reviews)\n",
    "accuracy_afinn = accuracy_score(categories,predictions_afinn)\n",
    "print(f'ì •í™•ë„ :{accuracy_afinn:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b1acc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì •í™•ë„ :0.6\n"
     ]
    }
   ],
   "source": [
    "# 3. Vader\n",
    "def sentiment_vader(docs):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    return ['pos' if analyzer.polarity_scores(doc)['compound'] > 0 else 'neg' for doc in docs ]\n",
    "predictions_vader = sentiment_vader(reviews)\n",
    "accuracy_vader = accuracy_score(categories,predictions_vader)\n",
    "print(f'ì •í™•ë„ :{accuracy_vader:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d812aa",
   "metadata": {},
   "source": [
    "### ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ê°ì„± ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c17fb8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ê°ì„±ë¶„ì„\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # ë²¡í„°í™” íˆ´\n",
    "from sklearn.naive_bayes import MultinomialNB # ëª¨ë¸ 1\n",
    "from sklearn.linear_model import LogisticRegression # ëª¨ë¸ 2\n",
    "from sklearn.model_selection import train_test_split # ë°ì´í„° ë¶„ë¦¬\n",
    "from sklearn.metrics import classification_report # ì„±ëŠ¥ ì¡°íšŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bafc114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ\n",
    "\n",
    "# ë² ì´ì¦ˆ ì •ë¦¬\n",
    "# \"ì¢‹ë‹¤\" ë‹¨ì–´ë¥¼ ë³¸í›„ ì´ ë¦¬ë·°ê°€ ê¸ì •ì¼ í™•ë¥ \n",
    "# p(ê¸ì • | \"ì¢‹ë‹¤\") = p(\"ì¢‹ë‹¤\" | ê¸ì •) x p(ê¸ì •) / p(\"ì¢‹ë‹¤\")\n",
    "    # \"ì¢‹ë‹¤\"ê°€ ê¸ì •ì¼ í™•ë¥  = ê¸ì •ì¤‘ì— \"ì¢‹ë‹¤\"ê°€ ìˆëŠ” í™•ë¥  x ì „ì²´ ë¦¬ë·°ì¤‘ ê¸ì • ë¦¬ë·°  ë¹„ìœ¨ / ì „ì²´ë°ì´í„°ì—ì„œ \"ì¢‹ë‹¤\" ë“±ì¥ ë¹„ìœ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "911264f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 80)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë°ì´í„° ë¶„í• \n",
    "dataset = train_test_split(reviews, categories, test_size=0.2, random_state=42, stratify=categories)\n",
    "len(dataset[0]),len(dataset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7991b784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf ë²¡í„°í™”\n",
    "vectorizer = TfidfVectorizer(max_features = 1000)\n",
    "# tf-idf ë²¡í„°í™”\n",
    "vectorizer = TfidfVectorizer(max_features = 1000)\n",
    "\n",
    "# í•™ìŠµ ë°ì´í„°ë¡œ fit + transform\n",
    "x_train = vectorizer.fit_transform(dataset[0])\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” transformë§Œ\n",
    "x_test = vectorizer.transform(dataset[1])\n",
    "\n",
    "# \n",
    "y_train = dataset[2]\n",
    "y_test = dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "43b2b490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.75      0.60      0.67        10\n",
      "         pos       0.67      0.80      0.73        10\n",
      "\n",
      "    accuracy                           0.70        20\n",
      "   macro avg       0.71      0.70      0.70        20\n",
      "weighted avg       0.71      0.70      0.70        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MultinomialNB\n",
    "mnb_clf = MultinomialNB()\n",
    "mnb_clf.fit(x_train,y_train)\n",
    "predict = mnb_clf.predict(x_test)\n",
    "print(classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b16a777d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.57      0.80      0.67        10\n",
      "         pos       0.67      0.40      0.50        10\n",
      "\n",
      "    accuracy                           0.60        20\n",
      "   macro avg       0.62      0.60      0.58        20\n",
      "weighted avg       0.62      0.60      0.58        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train,y_train)\n",
    "predict = lr.predict(x_test)\n",
    "print(classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756bb9bb",
   "metadata": {},
   "source": [
    "### ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ê°ì„±ë¶„ì„ ì„±ëŠ¥ ì˜¬ë¦¬ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fe72a4",
   "metadata": {},
   "source": [
    "#### ë¨¸ì‹ ëŸ¬ë‹ ê°ì„±ë¶„ì„ ì„±ëŠ¥ í–¥ìƒ ìš”ì†Œ, ë°ì´í„° ì „ì²˜ë¦¬ì™€ ë²¡í„°í™” í•­ëª©\n",
    " - ê·¸ì™¸ ê³ ë ¤í•­ëª© : ë°ì´í„° ì¸¡ë©´, ëª¨ë¸ ì„ íƒ, í•˜ì´í¼íŒŒë¦¬ë¯¸í„° ë° í•™ìŠµ\n",
    " https://chatgpt.com/s/t_6912a19b73688191a7195375199906c6\n",
    "\n",
    "#### 1ï¸âƒ£ ì „ì²˜ë¦¬(Preprocessing) ì „ëµ\n",
    "\n",
    "1. **í† í°í™”(Tokenization)**\n",
    "\n",
    "   * í•œêµ­ì–´: í˜•íƒœì†Œ ë‹¨ìœ„(Okt, Mecab, Komoran ë“±) vs. ë‹¨ì–´ ë‹¨ìœ„\n",
    "   * ì˜ì–´: ë‹¨ì–´ ë‹¨ìœ„, ì„œë¸Œì›Œë“œ(BPE, WordPiece)\n",
    "   * ê°ì„± ë¶„ì„ì—ì„œëŠ” **ì˜ë¯¸ ë‹¨ìœ„ ìœ ì§€**ê°€ ì¤‘ìš” â†’ í˜•íƒœì†Œ ë‹¨ìœ„ ì¶”ì²œ\n",
    "\n",
    "2. **ë¶ˆìš©ì–´ ì œê±°(Stopword Removal)**\n",
    "\n",
    "   * â€˜ì€â€™, â€˜ëŠ”â€™, â€˜ì´â€™, â€˜ê°€â€™, â€˜ê·¸ë¦¬ê³ â€™, â€˜butâ€™, â€˜theâ€™ ë“±\n",
    "   * ë‹¨, ì¼ë¶€ ê°ì„± í‘œí˜„ì— ì¤‘ìš”í•œ ë‹¨ì–´ëŠ” ì œì™¸í•˜ì§€ ì•Šë„ë¡ ì£¼ì˜\n",
    "\n",
    "3. **ì •ê·œí™”(Normalization)**\n",
    "\n",
    "   * ëŒ€ì†Œë¬¸ì í†µì¼, ê³µë°± ì œê±°, íŠ¹ìˆ˜ë¬¸ì/ì´ëª¨ì§€ ì²˜ë¦¬\n",
    "   * ê°ì„± ì´ëª¨ì§€ ğŸ˜, ğŸ˜¡ ëŠ” ê¸ì •/ë¶€ì • ì •ë³´ê°€ ìˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ë‘˜ ìˆ˜ë„ ìˆìŒ\n",
    "   * ë°˜ë³µ ë¬¸ì ì •ê·œí™”: `ì¢‹ì•„ìš”!!!` â†’ `ì¢‹ì•„ìš”!`\n",
    "\n",
    "4. **ì–´ê°„ ì¶”ì¶œ(Stemming) / í‘œì œì–´ ì¶”ì¶œ(Lemmatization)**\n",
    "\n",
    "   * ë™ì‚¬/í˜•ìš©ì‚¬ í™œìš©í˜• í†µì¼: â€˜ì¢‹ë‹¤â€™, â€˜ì¢‹ì•„ìš”â€™, â€˜ì¢‹ì•˜ìŒâ€™ â†’ `ì¢‹ë‹¤`\n",
    "   * ê°ì„± ë‹¨ì–´ì˜ ë‹¤ì–‘í•œ ë³€í˜• í†µí•© â†’ ë¹ˆë„ ê¸°ë°˜ ë²¡í„°í™” íš¨ê³¼ ìƒìŠ¹\n",
    "\n",
    "5. **n-gram ìƒì„±**\n",
    "\n",
    "   * ë‹¨ì–´ ë‹¨ìœ„ bigram/trigram í™œìš© â†’ ë¬¸ë§¥ ë°˜ì˜\n",
    "   * ì˜ˆ: `not good` â†’ ë‘ ë‹¨ì–´ë¥¼ í•¨ê»˜ ê³ ë ¤í•˜ì—¬ ë¶€ì • ì˜ë¯¸ ìœ ì§€\n",
    "\n",
    "6. **íŠ¹ìˆ˜ ê¸°í˜¸Â·ìˆ«ì ì²˜ë¦¬**\n",
    "\n",
    "   * ê°ì„± ì •ë³´ì™€ ê´€ë ¨ ì—†ëŠ” ìˆ«ì ì œê±°\n",
    "   * ì´ëª¨ì§€, ê°•ì¡° ê¸°í˜¸(`!!!`, `??`)ëŠ” ë³„ë„ë¡œ featureë¡œ í™œìš© ê°€ëŠ¥\n",
    "\n",
    "---\n",
    "\n",
    "#### 2ï¸âƒ£ ë²¡í„°í™”(Feature) ì „ëµ\n",
    "\n",
    "1. **ê¸°ì´ˆ ë²¡í„°í™”**\n",
    "\n",
    "   * **Bag-of-Words (BOW)**: ë‹¨ì–´ ì¶œí˜„ ë¹ˆë„ ê¸°ë°˜\n",
    "   * **TF-IDF**: ìì£¼ ë“±ì¥í•˜ì§€ë§Œ ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ ì˜í–¥ ê°ì†Œ\n",
    "\n",
    "2. **n-gram ê¸°ë°˜ Feature**\n",
    "\n",
    "   * ë‹¨ì–´ ë‹¨ìœ„ bigram/trigram â†’ ë¶€ì •/ê¸ì • êµ¬ë¬¸ ìº¡ì²˜\n",
    "   * TF-IDF + n-gram ì¡°í•© ì‹œ ì„±ëŠ¥ ìƒìŠ¹ ê°€ëŠ¥\n",
    "\n",
    "3. **Word Embedding**\n",
    "\n",
    "   * **Word2Vec, GloVe, FastText**\n",
    "\n",
    "     * ë‹¨ì–´ ì˜ë¯¸ ë²¡í„° â†’ ìœ ì‚¬ ë‹¨ì–´ ì˜ë¯¸ ë°˜ì˜ ê°€ëŠ¥\n",
    "   * **FastText**: OOV(Out-of-Vocabulary) ë‹¨ì–´ ì²˜ë¦¬ ê°•ì \n",
    "\n",
    "4. **ë¬¸ì¥/ë¬¸ë§¥ Embedding**\n",
    "\n",
    "   * **BERT, KoBERT, Sentence-BERT**\n",
    "   * ë¬¸ì¥ ì „ì²´ ì˜ë¯¸ ë°˜ì˜ â†’ ë‹¨ìˆœ ë‹¨ì–´ ë²¡í„°ë³´ë‹¤ ê°ì • nuance ì¡ê¸° ì¢‹ìŒ\n",
    "\n",
    "5. **ê°ì„± ì‚¬ì „ ê¸°ë°˜ Feature**\n",
    "\n",
    "   * ê¸ì •/ë¶€ì • ë‹¨ì–´ ì ìˆ˜ í•©ì‚° â†’ TF-IDF, Word Embeddingê³¼ í•¨ê»˜ ì‚¬ìš© ê°€ëŠ¥\n",
    "\n",
    "6. **íŠ¹ì§• ì„ íƒ/ì°¨ì› ì¶•ì†Œ**\n",
    "\n",
    "   * ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°, ìƒê´€ ì—†ëŠ” feature ì¤„ì´ê¸°\n",
    "   * PCA, TruncatedSVD ë“± â†’ sparse ë²¡í„° ì²˜ë¦¬ í›„ ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ’¡ **í•µì‹¬ ì „ëµ ìš”ì•½**\n",
    "\n",
    "* ì „ì²˜ë¦¬: í˜•íƒœì†Œ ë‹¨ìœ„ í† í°í™” + ë¶ˆìš©ì–´ ì œê±° + ì •ê·œí™” + ì–´ê°„ ì¶”ì¶œ + n-gram\n",
    "* ë²¡í„°í™”: TF-IDF + n-gram OR Word Embedding(BERT ê³„ì—´)\n",
    "* ê°ì„± ë¶„ì„ì—ì„œëŠ” **ë¬¸ë§¥ ë°˜ì˜ + ë¶€ì • ì²˜ë¦¬**ê°€ ê°€ì¥ ì¤‘ìš”í•œ ìš”ì†Œ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b0fc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„±ëŠ¥ í–¥ìƒ ìš”ì†Œ\n",
    "# ì†Œë¬¸ì ë³€í™˜ - ì¤‘ë³µë‹¨ì–´ ì¤„ì´ê¸°\n",
    "# ì—°ì†ëœ ë¬¸ìì—´ ì¤‘ì— 3ê¸€ì ì´ìƒìœ¼ë¡œ ì§€ì • - ì˜ë¯¸ì—†ëŠ” ì¡°ì‚¬ë‚˜ ì ‘ë¯¸ì‚¬ ê±¸ëŸ¬ë‚´ê¸°\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86673cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata2\\miniconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ì»¤ìŠ¤í…€ í† í¬ë‚˜ì´ì € í•¨ìˆ˜ ë§Œë“¤ê¸°\n",
    "def custom_tokenizer(text):\n",
    "    text = text.lower()\n",
    "    tokenizer =  RegexpTokenizer(r\"[\\w']{3,}\")\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    porter = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [porter.stem(token) for token in tokens if token not in stop_words]\n",
    "vector = TfidfVectorizer(\n",
    "    tokenizer  = custom_tokenizer\n",
    "    ,max_features=1000\n",
    "    ,min_df=5\n",
    "    ,max_df=0.5\n",
    "    #, token_pattern = r\"[\\w']{3,}\"\n",
    "    # tokenizerë¥¼ ì§ì ‘ ì§€ì •í•˜ë©´ TfidfVectorizerëŠ” token_patternì„ ë¬´ì‹œ\n",
    ")\n",
    "x_train = vector.fit_transform(dataset[0])\n",
    "x_test = vector.transform(dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c257746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ í•™ìŠµ ë° ì„±ëŠ¥ ì¡°íšŒ í•¨ìˆ˜\n",
    "def evaluate_model(model):    \n",
    "    model.fit(x_train,y_train)\n",
    "    predict = model.predict(x_test)\n",
    "    print( classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "629d7d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.80      0.80      0.80        10\n",
      "         pos       0.80      0.80      0.80        10\n",
      "\n",
      "    accuracy                           0.80        20\n",
      "   macro avg       0.80      0.80      0.80        20\n",
      "weighted avg       0.80      0.80      0.80        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a27678d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.75      0.60      0.67        10\n",
      "         pos       0.67      0.80      0.73        10\n",
      "\n",
      "    accuracy                           0.70        20\n",
      "   macro avg       0.71      0.70      0.70        20\n",
      "weighted avg       0.71      0.70      0.70        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(MultinomialNB())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
