{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7662e00d",
   "metadata": {},
   "source": [
    "ë²ˆì—­"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93550373",
   "metadata": {},
   "source": [
    "```\n",
    "í•™ìŠµìš© ì˜ì–´-í”„ë‘ìŠ¤ì–´ ë³‘ë ¬ ë¬¸ì¥ ë°ì´í„° ì¤€ë¹„\n",
    "ê°œë…: \n",
    "   - ì…ë ¥(ì˜ì–´)ê³¼ ì¶œë ¥(í”„ë‘ìŠ¤ì–´) ìŒìœ¼ë¡œ êµ¬ì„±\n",
    "   - ë””ì½”ë” ì…ë ¥ì—ëŠ” ì‹œì‘ í† í°(\\t), íƒ€ê²Ÿì—ëŠ” ì¢…ë£Œ í† í°(\\n)\n",
    "ì¶”ê°€ ì„¤ëª…:\n",
    "   - input_texts: ì¸ì½”ë”ì— ì…ë ¥ë  ì˜ì–´ ë¬¸ì¥\n",
    "   - target_texts: ë””ì½”ë”ê°€ ìƒì„±í•´ì•¼ í•  í”„ë‘ìŠ¤ì–´ ë¬¸ì¥ (ì „ì²˜ë¦¬ í¬í•¨)\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2296f0",
   "metadata": {},
   "source": [
    "#### ë¼ì´ë¸ŒëŸ¬ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6196fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ : Hello                --> í‹°ì¼“ : \tBonjour\n",
      "\n",
      "ì…ë ¥ : How are you          --> í‹°ì¼“ : \tComment allez-vous\n",
      "\n",
      "ì…ë ¥ : Good morning         --> í‹°ì¼“ : \tBonjour matin\n",
      "\n",
      "ì…ë ¥ : Thank you            --> í‹°ì¼“ : \tMerci\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c0318c",
   "metadata": {},
   "source": [
    "#### ì¸ì½”ë”Â·ë””ì½”ë”ìš© ë¬¸ì ì‚¬ì „ êµ¬ì¶• ë° ë°ì´í„° ì¸ë±ì‹± ì¤€ë¹„ ë‹¨ê³„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c537aece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°\n",
    "data_pairs = [\n",
    "    (\"Hello\", \"Bonjour\"),\n",
    "    (\"How are you\", \"Comment allez-vous\"),\n",
    "    (\"Good morning\", \"Bonjour matin\"),\n",
    "    (\"Thank you\", \"Merci\"),\n",
    "]\n",
    "# ì…ë ¥ê³¼ íƒ€ê²Ÿì„ ë¶„ë¦¬\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "for eng, fra in data_pairs: # ì˜ì–´ë¡œ í”„ë‘ìŠ¤ì–´ ìˆœí™˜\n",
    "    input_texts.append(eng) # ì˜ì–´ëŠ” input_textì— ì¶”ê°€\n",
    "    # ë””ì½”ë” ì…ë ¥ '\\t' (ì‹œì‘)\n",
    "    # ë””ì½”ë” ì¶œë ¥ : '\\n' (ì¢…ë£Œ)\n",
    "    # ì´ìŠ¤ì¼€ì´í”„ë¬¸ìë¡œ ë¬¸ì¥ì˜ ì‹œì‘ê³¼ ë¬¸ì¥ì˜ ì¢…ë¥˜ë¥¼ í‘œì‹œí•œë‹¤.  ^ ê·¸ ì´ìœ  ^\n",
    "    target_texts.append(f'\\t{fra}\\n')\n",
    "    \n",
    "# ì…ë ¥ ë¬¸ì¥ê³¼ í•´ë‹¹ íƒ€ê¹ƒ(ë²ˆì—­ë¬¸) ìŒì„ í™•ì¸ _\n",
    "for i in range(len(input_texts)):\n",
    "    print(f\"ì…ë ¥ : {input_texts[i]:20s} --> í‹°ì¼“ : {target_texts[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76d2140",
   "metadata": {},
   "source": [
    "- ë¬¸ì ë‹¨ìœ„ ì‚¬ì „(vocabulary) ìƒì„± ë° ì •ìˆ˜ ì¸ë±ìŠ¤ ë³€í™˜\n",
    "- ê°œë…:\n",
    "    - ê° ë¬¸ìë¥¼ ê³ ìœ í•œ ì •ìˆ˜ë¡œ ë§¤í•‘\n",
    "    - ì…ë ¥ê³¼ íƒ€ê²Ÿì˜ ì‚¬ì „ì€ ë³„ë„ ê´€ë¦¬\n",
    "    - ì›-í•« ì¸ì½”ë”©ìœ¼ë¡œ ì‹ ê²½ë§ ì…ë ¥ í˜•íƒœ ìƒì„±\n",
    "- ì„¤ëª…:\n",
    "    - input_characters: ì˜ì–´ ë¬¸ì¥ì— ë“±ì¥í•˜ëŠ” ëª¨ë“  ê³ ìœ  ë¬¸ì\n",
    "    - target_characters: í”„ë‘ìŠ¤ì–´ ë¬¸ì¥ + íŠ¹ìˆ˜ í† í°(\\t, \\n)\n",
    "    - encoder_input_data: 3D ë°°ì—´ (ìƒ˜í”Œ, ì‹œí€€ìŠ¤ ê¸¸ì´, ë¬¸ì ì‚¬ì „ í¬ê¸°)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8005b765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì ì¸ë±ì‹± ë° ì›-í•« ì¸ì½”ë”©\n",
    "\n",
    "# ë¬¸ì(character) ë‹¨ìœ„ë¡œ ì¶”ì¶œ\n",
    "# input_characters = set()\n",
    "# target_characters  = set()\n",
    "\n",
    "# set() ì§‘í•©ìœ¼ë¡œ íƒ€ì…ë³€í™˜í•˜ëŠ” í•¨ìˆ˜, ì¤‘ë³µì„ ì—†ì•¤ë‹¤.\n",
    "\n",
    "# ë°©ë²• 1)\n",
    "# for text in input_texts:\n",
    "#     for char in text:\n",
    "#         input_characters.add(char)\n",
    "\n",
    "# ë°©ë²• 2)\n",
    "input_characters = {char for text in input_texts for char in text} # ^\n",
    "target_characters = {char for target_texts in target_texts for char in target_texts} # ^\n",
    "# {} ì§‘í•©íƒ€ì…ìœ¼ë¡œ ìƒì„±, ì¤‘ë³µ ì—†ì• ê¸°\n",
    "# ìœ„ ë°©ë²• 1ì„ ì„œìˆœëŒ€ë¡œ ë‚˜ì—´í•œë‹¤ëŠ” ëŠë‚Œìœ¼ë¡œ\n",
    "\n",
    "# ì •ë ¬í•´ì„œ ì¸ë±ìŠ¤ì˜ ìˆœì„œì˜ ì¼ê´€ì„± í™•ë³´\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "\n",
    "# ì…ë ¥, ì¶œë ¥ ë¬¸ììˆ˜ ì¡°íšŒ\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "\n",
    "# ê°€ì¥ ê¸´ ë¬¸ì¥ ê¸¸ì´ ì¡°íšŒ, íŒ¨ë”© ê¸°ì¤€ í™•ë¦½\n",
    "max_encoder_seq_length = max(len(txt) for txt in input_texts)\n",
    "max_decoder_seq_length = max(len(txt) for txt in target_texts)\n",
    "\n",
    "# ë¬¸ì -> ì¸ë±ìŠ¤ ë§¤í•‘ : ë”•ì…”ë„ˆë¦¬ë¡œ ìƒì„±\n",
    "input_token_index = { char : i for  i, char in enumerate(input_characters)}\n",
    "target_token_index = { char : i for  i, char in enumerate(target_characters)}\n",
    "\n",
    "# ì¸ë±ìŠ¤ -> ë¬¸ì ì—­ë§¤í•‘ (ì¶”ë¡ ì‹œ ì‚¬ìš©)\n",
    "reversed_input_token_index = {idx : char for char,idx, in input_token_index.items()}\n",
    "reversed_target_token_index = {idx : char for char,idx, in target_token_index.items()}\n",
    "\n",
    "# encoder_input_data: 3D ë°°ì—´ (ìƒ˜í”Œ, ì‹œí€€ìŠ¤ ê¸¸ì´, ë¬¸ì ì‚¬ì „ í¬ê¸°) ì´ê±° ë§Œë“¤ê¸°.\n",
    "encoder_input_data = np.zeros((len(input_texts),max_encoder_seq_length,num_encoder_tokens),\n",
    "                              dtype='float32')\n",
    "# encoder_input_data = np.zeros((10,20,30)) -> encoder_input_data.shape -> (10,20,30) \n",
    "decoder_input_data = np.zeros((len(input_texts),max_decoder_seq_length,num_decoder_tokens),\n",
    "                              dtype='float32')\n",
    "decoder_target_data = np.zeros((len(target_texts),max_decoder_seq_length,num_decoder_tokens),\n",
    "                              dtype='float32')\n",
    "# decoder_input_dataì™€ decoder_target_data ë¥¼ ë§Œë“œëŠ” ì´ìœ  ^"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c70f5a6",
   "metadata": {},
   "source": [
    "| êµ¬ë¶„                    | ì—­í•                       | ì˜ˆì‹œ (ëª©í‘œ: \"Bonjour\") |\n",
    "| --------------------- | ----------------------- | ------------------ |\n",
    "| `encoder_input_data`  | ì…ë ¥ ë¬¸ì¥ (ì˜ì–´)              | \"Hello\"            |\n",
    "| `decoder_input_data`  | ë””ì½”ë”ê°€ ì°¸ê³ í•˜ëŠ” â€œì§€ê¸ˆê¹Œì§€ì˜ ë²ˆì—­ ê²°ê³¼â€ | \"\\tBonjour\"        |\n",
    "| `decoder_target_data` | ë””ì½”ë”ê°€ ì˜ˆì¸¡í•´ì•¼ í•  â€œë‹¤ìŒ ê¸€ì ì •ë‹µâ€  | \"Bonjour\\n\"        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf4e6183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê³ ìœ  ì…ë ¥ ë¬¸ììˆ˜ : 12\n",
      "ê³ ìœ  íƒ€ê²Ÿ ë¬¸ììˆ˜ : 20\n",
      "ìµœëŒ€ ì…ë ¥ ë¬¸ì¥ ê¸¸ì´ : 12\n",
      "ìµœëŒ€ íƒ€ê²Ÿ ë¬¸ì¥ ê¸¸ì´ : 20\n",
      "# ìƒ˜í”Œ, ì‹œí€€ìŠ¤ ê¸¸ì´, ë¬¸ì ì‚¬ì „ í¬ê¸°\n",
      "encoder_input_data : (4, 12, 19)\n",
      "decoder_input_data : (4, 20, 22)\n",
      "decoder_target_data : (4, 20, 22)\n"
     ]
    }
   ],
   "source": [
    "# ë¬¸ìë³„ ì› í•« ì¸ì½”ë”©\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "        \n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_input_data (ë””ì½”ë” ì…ë ¥) : ì „ì²´ íƒ€ê²Ÿ ì‹œí€€ìŠ¤ (ì‹œì‘ í† í° í¬í•¨)\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "        \n",
    "        # decoder_target_Data (ë””ì½”ë” ì¶œë ¥) : í•œ íƒ€ì„ìŠ¤í… ì•ì„  ì •ë‹µ (Teacher Force ìš©)\n",
    "        # ë””ì½”ë” ì…ë ¥ \\tì•ˆë…•\n",
    "        # ë””ì½”ë” ì¶œë ¥ ì•ˆë…•\\n\n",
    "        # í•œìŠ¤í… ì‹œí”„íŠ¸ -Teacher Forcing\n",
    "        \n",
    "        # decoder_target_data: í•œ íƒ€ì„ìŠ¤í… ì•ì„  ì •ë‹µ (Teacher Forcingìš©)\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "# hi ì…ë ¥\n",
    "# \\t hello  ë””ì½”ë” ì…ë ¥\n",
    "# hello \\n  ë””ì½”ë” ì¶œë ¥ - Teacher Forcing í•œ ìŠ¤í… ì•ìœ¼ë¡œ ì´ë™\n",
    "print(f'ê³ ìœ  ì…ë ¥ ë¬¸ììˆ˜ : {max_encoder_seq_length}')\n",
    "print(f'ê³ ìœ  íƒ€ê²Ÿ ë¬¸ììˆ˜ : {max_decoder_seq_length}')\n",
    "print(f'ìµœëŒ€ ì…ë ¥ ë¬¸ì¥ ê¸¸ì´ : {max_encoder_seq_length}')\n",
    "print(f'ìµœëŒ€ íƒ€ê²Ÿ ë¬¸ì¥ ê¸¸ì´ : {max_decoder_seq_length}')\n",
    "print('# ìƒ˜í”Œ, ì‹œí€€ìŠ¤ ê¸¸ì´, ë¬¸ì ì‚¬ì „ í¬ê¸°')\n",
    "print(f'encoder_input_data : {encoder_input_data.shape}')\n",
    "print(f'decoder_input_data : {decoder_input_data.shape}')\n",
    "print(f'decoder_target_data : {decoder_target_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdb5f98",
   "metadata": {},
   "source": [
    "- LSTM ê¸°ë°˜ Seq2Seq ì¸ì½”ë”-ë””ì½”ë” í•™ìŠµ ëª¨ë¸ êµ¬ì¶•\n",
    "- ê°œë…:\n",
    "    - Encoder: ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•˜ê³  ìµœì¢… ìƒíƒœ(h, c) ì¶œë ¥\n",
    "    - Decoder: Encoder ìƒíƒœë¥¼ ì´ˆê¸°ê°’ìœ¼ë¡œ ë°›ì•„ íƒ€ê²Ÿ ì‹œí€€ìŠ¤ ìƒì„±\n",
    "    - return_state=True: LSTM ë‚´ë¶€ ìƒíƒœ(h, c) ë°˜í™˜\n",
    "    - return_sequences=True: ëª¨ë“  íƒ€ì„ìŠ¤í… ì¶œë ¥\n",
    "- ì„¤ëª…:\n",
    "    - encoder_states: [h, c] (hidden state, cell state)\n",
    "    - decoder_lstm: ì´ˆê¸° ìƒíƒœë¡œ encoder_states ì „ë‹¬\n",
    "    - decoder_dense: Softmaxë¡œ ê° íƒ€ì„ìŠ¤í…ì˜ ë¬¸ì í™•ë¥  ë¶„í¬ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e6d462",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "latent_dim = 256  # LSTM ì€ë‹‰ ì°¨ì› (ë‚´ë¶€ í‘œí˜„ í¬ê¸°)\n",
    "\n",
    "# ==================== Encoder ====================\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens), name='encoder_input')\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True, name='encoder_lstm')\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "# encoder_outputsëŠ” ì‚¬ìš©í•˜ì§€ ì•Šê³ , ë‚´ë¶€ ìƒíƒœ(state_h, state_c)ë§Œ ë””ì½”ë”ë¡œ ì „ë‹¬\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# ==================== Decoder ====================\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens), name='decoder_input')\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "\n",
    "# ë””ì½”ë” ì´ˆê¸° ìƒíƒœë¡œ ì¸ì½”ë” ìµœì¢… ìƒíƒœ ì‚¬ìš© (ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "# ê° íƒ€ì„ìŠ¤í…ì—ì„œ ë¬¸ì í™•ë¥  ë¶„í¬ ìƒì„±\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# ==================== í•™ìŠµ ëª¨ë¸ ====================\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs, name='seq2seq_training')\n",
    "\n",
    "print(\"\\nğŸ—ï¸ ëª¨ë¸ êµ¬ì¡°:\")\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
