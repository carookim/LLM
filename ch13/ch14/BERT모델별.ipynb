{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d46345ce",
   "metadata": {},
   "source": [
    "```\n",
    "BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "ì—¬ê¸° ''ì•ˆì— ë‹¤ë¥¸ ëª¨ë¸ìœ¼ ë¶ˆëŸ¬ ì˜¬ ìˆ˜ ìˆë‹¤ëŠ” ê±¸ì•Œê² ì–´\n",
    "ê·¼ë°, ê·¸ê±° ë§ˆë‹¤ ë­ê°€ ë‹¤ë¥¸ê±°ì•¼?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e753b0",
   "metadata": {},
   "source": [
    "ì¢‹ì•„ìš”, ì—¬ê¸°ì„œ ì¤‘ìš”í•œ í¬ì¸íŠ¸ëŠ” **BERT í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ì´ë¦„ì´ ë‹¤ë¥´ë©´ ë‚´ë¶€ êµ¬ì¡°ì™€ í•™ìŠµ ë°ì´í„°, ëª©ì ì´ ë‹¬ë¼ì§„ë‹¤**ëŠ” ê±°ì˜ˆìš”. í•˜ë‚˜ì”© ì‚´í´ë³¼ê²Œìš”. ğŸ˜„\n",
    "\n",
    "---\n",
    "\n",
    "### 1ï¸âƒ£ BERT ê³„ì—´\n",
    "\n",
    "* `'bert-base-uncased'`, `'bert-large-uncased'`, `'bert-base-cased'` ë“±\n",
    "* **base vs large**\n",
    "\n",
    "  * `base`: 12ê°œ ë ˆì´ì–´, 768 hidden, 12 attention head â†’ ê°€ë²¼ì›€, ë¹ ë¦„\n",
    "  * `large`: 24ê°œ ë ˆì´ì–´, 1024 hidden, 16 attention head â†’ ë” ì •í™•í•˜ì§€ë§Œ ëŠë¦¼\n",
    "* **uncased vs cased**\n",
    "\n",
    "  * `uncased`: ì†Œë¬¸ìë¡œ í†µì¼, ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ\n",
    "  * `cased`: ëŒ€ì†Œë¬¸ì êµ¬ë¶„, ì´ë¦„ ë“± ì •í™•í•œ ì¼€ì´ìŠ¤ í•„ìš”í•  ë•Œ ì‚¬ìš©\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ ë‹¤ë¥¸ ë³€í˜• ëª¨ë¸\n",
    "\n",
    "1. **RoBERTa**\n",
    "\n",
    "   * `'roberta-base'`, `'roberta-large'`\n",
    "   * BERTì™€ êµ¬ì¡° ê±°ì˜ ê°™ì§€ë§Œ, í•™ìŠµ ë°ì´í„°ì™€ í† í¬ë‚˜ì´ì €ê°€ ë‹¤ë¦„\n",
    "   * ë” ê¸´ ë¬¸ì¥ í•™ìŠµ ê°€ëŠ¥, ì„±ëŠ¥ ì¡°ê¸ˆ ë” ì¢‹ìŒ\n",
    "2. **DistilBERT**\n",
    "\n",
    "   * `'distilbert-base-uncased'`\n",
    "   * BERTë¥¼ ì••ì¶•í•œ ëª¨ë¸ â†’ ì†ë„ ë¹ ë¥´ê³  ë©”ëª¨ë¦¬ ì ê²Œ ì‚¬ìš©, ì„±ëŠ¥ ì¡°ê¸ˆ ë–¨ì–´ì§\n",
    "3. **ALBERT**\n",
    "\n",
    "   * `'albert-base-v2'`\n",
    "   * íŒŒë¼ë¯¸í„° ê³µìœ , ê°€ë³ê²Œ í•™ìŠµ, í° ëª¨ë¸ì—ë„ ë¹ ë¦„\n",
    "4. **í•œêµ­ì–´ ëª¨ë¸**\n",
    "\n",
    "   * `'kykim/bert-kor-base'`, `'monologg/kobert'`\n",
    "   * í•œêµ­ì–´ ì „ìš© í† í¬ë‚˜ì´ì €ì™€ ë‹¨ì–´ ì‚¬ì „ ì‚¬ìš©\n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ ëª¨ë¸ ì„ íƒ ê¸°ì¤€\n",
    "\n",
    "* **ì†ë„ vs ì„±ëŠ¥** â†’ base vs large, distilBERT\n",
    "* **ì–¸ì–´** â†’ uncased/cased, ë‹¤êµ­ì–´ vs í•œêµ­ì–´\n",
    "* **ë©”ëª¨ë¦¬ ì œí•œ** â†’ ì•Œë²„íŠ¸, DistilBERT\n",
    "* **íŠ¹ì • ëª©ì ** â†’ ê°ì„±ë¶„ì„, NER ë“± task-specific ëª¨ë¸ë„ ìˆìŒ\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ’¡ **ì •ë¦¬í•˜ë©´**\n",
    "`from_pretrained('ëª¨ë¸ì´ë¦„')`ì—ì„œ ëª¨ë¸ ì´ë¦„ë§Œ ë°”ê¾¸ë©´\n",
    "\n",
    "* í† í¬ë‚˜ì´ì € ê·œì¹™\n",
    "* ì‚¬ì „(vocab)\n",
    "* í•™ìŠµëœ íŒŒë¼ë¯¸í„°\n",
    "* ì„±ëŠ¥ê³¼ ì†ë„\n",
    "  ëª¨ë‘ ë‹¬ë¼ì§„ë‹¤ê³  ë³´ë©´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "**ì¡°íšŒë°©ë²•**\n",
    "\n",
    "```1 ) HuggingFace ëª¨ë¸ í—ˆë¸Œ```\n",
    " - URL: https://huggingface.co/models\n",
    " - ê²€ìƒ‰ì°½ì— 'bert', 'roberta', 'kobert' ë“± ì…ë ¥\n",
    " - ëª¨ë¸ ìƒì„¸ í˜ì´ì§€ì—ì„œ:\n",
    " - Tokenizer ì¢…ë¥˜\n",
    "   - Layer / Hidden size / Parameters\n",
    "   - Pretraining ë°ì´í„°\n",
    "   - Language / Task ìš©ë„\n",
    "   - ì˜ˆ: BERT-base-uncased\n",
    "```2) ì§ì ‘ì¡°íšŒ```\n",
    "```python\n",
    "# tokenizer ì¡°íšŒê¸°ëŠ¥ : .vocab_size, .model_max_length\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(tokenizer.vocab_size)   # ë‹¨ì–´ ì‚¬ì „ í¬ê¸°\n",
    "print(tokenizer.model_max_length)  # ìµœëŒ€ í† í° ê¸¸ì´\n",
    "\n",
    "# ëª¨ë¸êµ¬ì¡°ë„ í™•ì¸ê°€ëŠ¥\n",
    "from transformers import BertModel\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "print(model.config)  # layer, hidden size, head ìˆ˜ ë“±\n",
    "\n",
    "```\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
