{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37d05b22",
   "metadata": {},
   "source": [
    "Attention Mechanism\n",
    "\n",
    "고양이가 잔다 -> the cat sleeps\n",
    "\n",
    "입력 : \"고양이가 잔다.\"\n",
    "\n",
    "[전처리] -> <start> 고양이가 잔다 <end>\n",
    "\n",
    "[ENCODER] -> 각 단어마다 hidden state 생성\n",
    "\n",
    "[ATTENTION] -> 어디에 집중할지 계산\n",
    "\n",
    "[DECODER] -> 한단어씩 생성\n",
    "\n",
    "출력 : the cat sleeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52bc0c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본\n",
    "입력 = \"고양이가 자요\"\n",
    "# 1단계 전처리\n",
    "전처리_후 =f'<start> {input} <end>'\n",
    "\n",
    "# 2단계 토큰화(단어->숫자)\n",
    "단어_사전 = {\n",
    "    '<pad>' : 0,\n",
    "    '<start>' : 1,\n",
    "    '<end>' : 2,\n",
    "    '고양이가' : 3,\n",
    "    '자요' : 4\n",
    "}\n",
    "\n",
    "# 최대길이 5대로\n",
    "정수_시퀀스 = [1,3,4,2,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca6955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1 : EMBEDDING(숫자 -> 벡터)\n",
    "# 임베딩 레이어 ========== 256차원\n",
    "정수_시퀀스 = [1,3,4,2,0]\n",
    "# Embedding.....\n",
    "임베딩_결과 = [\n",
    "    [0.2,-0.5, ..., 0.3], # 1 (<start>)의 벡터 (265)차원\n",
    "    [0.2,-0.5, ..., -0.2], # 3 (고양이가)의 벡터 (265)차원\n",
    "    [0.2,-0.5, ..., 0.1], # 4 (자요)의 벡터 (265)차원\n",
    "    [0.2,-0.5, ..., 0.3], # 2 (<end>)의 벡터 (265)차원\n",
    "    [0.2,-0.5, ..., -0.1]  # 0 (<pad>)의 벡터 (265)차원\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108a3ea7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras._tf_keras.keras' has no attribute 'layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m LSTM = \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayer\u001b[49m.LSTN()\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# STEP 2 : ENCODER(벡터 -> Hidden States)\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# LSTM Encoder\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 각 타임스탭마다 hidden state 생성\u001b[39;00m\n\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 초기상태\u001b[39;00m\n\u001b[32m      8\u001b[39m h0 = [\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m, ..., \u001b[32m0\u001b[39m] \u001b[38;5;66;03m# 512차원 영벡터\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata2\\miniconda3\\envs\\conda_venv\\Lib\\site-packages\\tensorflow\\python\\util\\lazy_loader.py:211\u001b[39m, in \u001b[36mKerasLazyLoader.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m    208\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` is not available with Keras 3.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    209\u001b[39m     )\n\u001b[32m    210\u001b[39m module = \u001b[38;5;28mself\u001b[39m._load()\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: module 'keras._tf_keras.keras' has no attribute 'layer'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "LSTM = tf.keras.layer.LSTN()\n",
    "# STEP 2 : ENCODER(벡터 -> Hidden States)\n",
    "# LSTM Encoder\n",
    "# 각 타임스탭마다 hidden state 생성\n",
    "\n",
    "# 초기상태\n",
    "h0 = [0,0,0, ..., 0] # 512차원 영벡터\n",
    "c0 = [0,0,0, ..., 0] # 512차원 영벡터\n",
    "\n",
    "# =================== 타입 스텝 1 : <start> 처리 ===================\n",
    "입력_t1 = [0.2,-0.5, ..., 0.3] # <start>의 임베딩\n",
    "h1 , c1 = LSTM(입력_t1, h0, c0)\n",
    "# =================== 타입 스텝 2 : 고양이가 처리 ===================\n",
    "입력_t2 = [0.2,-0.5, ..., -0.2] # 고양이가의 임베딩\n",
    "h2 , c2 = LSTM(입력_t2, h1, c1)\n",
    "# =================== 타입 스텝 3 : 자요가 처리 ===================\n",
    "입력_t3 = [0.2,-0.5, ..., 0.1] # 고양이가의 임베딩\n",
    "h3 , c3 = LSTM(입력_t3, h2, c2)\n",
    "# =================== 타입 스텝 4 : <end> 처리 ===================\n",
    "입력_t4 = [0.2,-0.5, ..., 0.3] # <end>의 임베딩\n",
    "h4 , c4 = LSTM(입력_t4, h3, c3)\n",
    "# =================== 타입 스텝 5 : <pad> 처리 ===================\n",
    "입력_t5 = [0.2,-0.5, ..., -0.1] # <pad>의 임베딩\n",
    "h5 , c5 = LSTM(입력_t5, h4, c4)\n",
    "# h5는 사용안함(마스킹)\n",
    "\n",
    "# =================== 인코더의 출력 ===================\n",
    "encoder_output  = [h1,h2,h3,h4,h5] # 모든 hidden states\n",
    "# 형태 [1,5,512] [배치, 시퀀스, units]\n",
    "encoder_fianl_h = h4 # 마지막 hidden state(디코더 초기화용)\n",
    "encoder_cinal_c = c4 # 마지막 cell state\n",
    "\n",
    "# ================= 정리 ==================\n",
    "#    ----> h1\n",
    "# 고양이가   --->   h2    \"고양이\"  정보를 압축\n",
    "# 자요       --->   h3    \"자요\"  정보를 압축\n",
    "#       --->  h4     전체 문장 요약\n",
    "#       --->  h5   무시됨\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
