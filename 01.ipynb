{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d15aa146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP = 사람의 언어를 컴퓨터가 이해하고, 생성하고, 활용하도록 만드는 기술"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81a24bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 토큰화 sent_tokenization\n",
    "# 기본로직\n",
    "    # 마침표, 느낌표, 물음표를 문장 끝 후보로 인식\n",
    "    # 약어 패턴 학습(Dr, Mr, U.S.A 등)\n",
    "    # 대문자로 시작하는지\n",
    "    # 통계적 모델을 사용해 진짜 문장 경계인지 판단\n",
    "# 다국어\n",
    "# 약어와 실제 문장끝을 구분하는 기계학습 모델 내장\n",
    "\n",
    "# 단어 토큰화 word_tokenization\n",
    "    # 공백 기준\n",
    "    # 구두점을 별도 토큰으로 분리\n",
    "    # 축약형 처리 it's -> it, s\n",
    "    # 소유격 처리 \"Let's\" Let, s\n",
    "\n",
    "    # 구두점 기반 WordPuncTokenizer\n",
    "    # 모든 구두점을 분리\n",
    "    # It's It, ', s\n",
    "\n",
    "    # 정규표현식 RegexpTokenizer\n",
    "    # ^\n",
    "\n",
    "# 노이즈와 불용어 제거\n",
    "    # set 집합 자료구조로 변환 : 중복제거\n",
    "    # List Comprehension : 불용어 필터링\n",
    "    # NLTK 불용어 사전 : 불용어 목록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c74fea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e75f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdade05",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('webtext')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('tagsets_json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddccbb9c",
   "metadata": {},
   "source": [
    "#### 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6771758",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Hello everyone. It's good to see you. Let's start our text minig class.\"\n",
    "sent_tokenize(sentence)\n",
    "# 리스트 형태로 문장별로 분리가 된것을 확인할 수 있다.\n",
    "# 이것도 일종의 모델로 , 비지도학습으로 만들어졌다고 한다.\n",
    "# 즉, 단순한 규칙 기반이 아니라 학습된 모델이기 때문에\n",
    "# \"Dr. Smith is here.\" 같은 경우에도 \"Dr.\" 를 문장 끝으로 오인하지 않아요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eb5800",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_kor = \"안녕하세요, 여러분. 만나서 반갑습니다. 이제 학습을 시작해 볼까요?\"\n",
    "sent_tokenize(sentence_kor)\n",
    "# 다국어도 지원하는 것을 확인할 수 있다.\n",
    "# 한국어 기능이 좋은지, 다국어로 러프하게 지원하는 건지 아닌지 ^"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6daf15",
   "metadata": {},
   "source": [
    "#### 단어 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be6703e",
   "metadata": {},
   "source": [
    "##### 공백 기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a743bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(sentence_kor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdefed4",
   "metadata": {},
   "source": [
    "##### 구두점 기반"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9664aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer # 클래스\n",
    "WordPunctTokenizer().tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20930a73",
   "metadata": {},
   "source": [
    "####  정규식 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8c85d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규식 토큰화\n",
    "# 정규식 토큰화? ^\n",
    "import re\n",
    "re.findall(\"[abc]\",sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a4be29",
   "metadata": {},
   "source": [
    "### 노이즈와 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59abb06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어, 불용어 목록 조회\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "english_stops = stopwords.words('english')\n",
    "text1 = \"Sorry, I couldn't go to movie yesterday.\"\n",
    "# tokens = word_tokenize(text1)\n",
    "\n",
    "#\n",
    "tokenizer = RegexpTokenizer(\"[\\\\w']+\")\n",
    "tokens = tokenizer.tokenize(text1.lower())\n",
    "\n",
    "# 리스트 컴프리헨션으로 불용어들을 걸러 리스트 구성하기\n",
    "[token for token in tokens if token not in english_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7588e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소문자 a~z로 이루어진 문자열에서 4글자 이상\n",
    "RegexpTokenizer(\"[a-z{4,}]\")\n",
    "\n",
    "# 3글자 이상\n",
    "RegexpTokenizer(\"[\\\\w']{3,}\")\n",
    "\n",
    "# 어포스트로피 ' 를 패턴에서 제외 can't can t\n",
    "RegexpTokenizer(\"[\\\\w]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eac3e9",
   "metadata": {},
   "source": [
    "### 어간 추출 Stemming\n",
    "- 줄기 stems : 단어에서 불필요한 요소를 제거하고 남는 핵심 형태\n",
    "- 단어는 다양한 형태를 가지고있다. 복수형 이나 과거형 과 같은 시제변환\n",
    "- 단어를 통일\n",
    "    - walk(걷다) walks walking walked --> 어간 walk로 통일\n",
    "    - 먹는다 먹었다 --> 먹- 으로 묶어서 컴퓨터가 같은 단어로 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194d567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PorterStemmer 규칙기바닝라서 완벽하지 못함 --> 속도가 빠름\n",
    "#                                              의미가 달라질수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c4f6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('cooking'), stemmer.stem('coockery'), stemmer.stem('cookbooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd0ff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LancasterStemmer 더 많은 규칙이 적용된다.\n",
    "#                  과도한 축약 위험이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dbeec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "stemmer.stem('cooking'), stemmer.stem('coockery'), stemmer.stem('cookbooks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e934035",
   "metadata": {},
   "source": [
    "### 표제어 추출 Lemmatization\n",
    "- Lemma 단어의 사전 기본형\n",
    "- 단어의 변형(시제, 복수/단수, 비교급)형태를 제거하고 사전(headword)에 나오는 정확한 원형으로 바꾸는 과정\n",
    "- 어간처럼 단어줄기가 아니라, 맥락과 품사를 고려한 올바른 형태를 추출한다.\n",
    "\n",
    "- better(비교급) -> good(원형)\n",
    "- 먹었다(동사-과거형) -> 먹다(동사원형)\n",
    "- 알고리즘 : 형태소 분석기(konlpy)를 사용해 품사(명사, 동사)를 보고 정확히 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f750ac50",
   "metadata": {},
   "source": [
    "##### 주요목적\n",
    "- 어간 추출처럼 대충 줄이지 않고 맥락에 맞느 정확한 단어로 만들어서 단어 추출, NLP 품질 향상\n",
    "- 단점 : 사전에 의존해서 언어 / 맥락 제한"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7176d98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('cooking')) # 기본이 명사로인식\n",
    "print(lemmatizer.lemmatize('cooking', pos = 'v')) # 품사를 동사(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df60de69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos 품사 설정 목록\n",
    "# n noun 명사 - 기본설정\n",
    "# v verb 동사\n",
    "# a adjact 형용사\n",
    "# r adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36bf8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize('better',pos='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5016a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표제어 변환에 어떤 한계가 있는지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d27deed",
   "metadata": {},
   "source": [
    "### 품사 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc128f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = \"hello everyone. It's good to see you. Let's start our text mining class.\"\n",
    "tokens = word_tokenize(tokens)\n",
    "nltk.pos_tag(tokens) # 품사 조회"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a08bcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('tagsets_json')\n",
    "# 품사 태그 정보 확인\n",
    "nltk.help.upenn_tagset('VB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bb93bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 품사 추출\n",
    "tag_lists = ['NN','VB','JJ'] # 추출할 품사 목록\n",
    "[word for word, tag in nltk.pos_tag(tokens) if tag in tag_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f568472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK는 영어기반  한국어의 조사분리 불가능, 어미변화처리 불가능\n",
    "# KoNlpy Okt 사용 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693cbd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b965b3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctrl + shift + p\n",
    "# Java: Configure Java Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b175eb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "t = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ab33d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = '''절망의 반대가 희망은 아니다.\n",
    "어두운 밤하늘에 별이 빛나듯\n",
    "희망은 절망 속에 싹트는 거지\n",
    "만약에 우리가 희망함이 적다면\n",
    "그 누가 세상을 비출어줄까.\n",
    "정희성, 희망 공부'''\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)\n",
    "print(nltk.pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d899d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NNP는 고유명사이다.\n",
    "# 대부분이 고유명사로 잘못인식되고있다.\n",
    "# 이는 사용하고있는 툴이 영어 중심이기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75558b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "t = Okt()\n",
    "print(f'형태소 : {t.morphs(sentence)}')\n",
    "print(f'명사 : {t.nouns(sentence)}')\n",
    "print(f'품사태깅 : {t.pos(sentence)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e6a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프와 워드클라우드\n",
    "# 데이터 불러오기\n",
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "gutenberg.fileids()\n",
    "doc_alice = gutenberg.open('carroll-alice.txt').read()\n",
    "print(doc_alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eeceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 및 전처리\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens_alice = word_tokenize(doc_alice)\n",
    "print(len(tokens_alice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f547a461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\\\w']{3,}\")\n",
    "reg_alice = tokenizer.tokenize(doc_alice.lower())\n",
    "print(len(reg_alice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045370e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_alice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b19ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거\n",
    "from nltk.corpus import stopwords\n",
    "english_stops = set(stopwords.words('english'))\n",
    "result_alice = [word for word in reg_alice if word not in english_stops]\n",
    "print(len(result_alice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7bbc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 품사 태깅 및 필터링\n",
    "# 명사 동사 형용사만 추출\n",
    "tag_lists = ['NN','VB', 'VBD', 'JJ'] # 추출할 품사 목록\n",
    "tagged_word = [word for word, tag in nltk.pos_tag(reg_alice) if tag in tag_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e5c071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 종복 횟수 확인 및 횟수를 딕셔너리에 저장\n",
    "from collections import Counter\n",
    "print(Counter(tagged_word))\n",
    "sorted_word_count = dict(Counter(tagged_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e68172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed99e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud().generate(doc_alice)\n",
    "plt.axis('off')\n",
    "plt.imshow(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b2408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = wordcloud.generate_from_frequencies(sorted_word_count)\n",
    "plt.axis('off')\n",
    "plt.imshow(wc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8662194c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4822fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영화 리뷰 데이터셋 로드\n",
    "import nltk\n",
    "nltk.download(\"movie_reviews\")\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9e2d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 데이터셋 구조 확인\n",
    "from nltk.corpus import movie_reviews\n",
    "print(f'review count : {len(movie_reviews.fileids())}')\n",
    "print(f'categories of reviews : {movie_reviews.categories()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25fbfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#BOW 카운트 백터 생성\n",
    "# 수동 구현\n",
    "documents =  [movie_reviews.words(fileid) for fileid in  movie_reviews.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7b5c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 단어 빈도 계산\n",
    "word_count = {}\n",
    "for text in documents:\n",
    "    for word in text:\n",
    "        word_count[word] = word_count.get(word,0) + 1 \n",
    "sorted_features = sorted(word_count, key=word_count.get,reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e05170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 및 재계산\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "tokenizer = RegexpTokenizer(\"[\\\\w']{3,}\")\n",
    "english_stops = set(stopwords.words('english'))\n",
    "documents = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()]\n",
    "tokens = [ [token for token tokenizer.tokenize(doc) if token not in english_stops]for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f13a102",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12687e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_count = {}\n",
    "for text in tokens:\n",
    "    for word in text:\n",
    "        word_count[word] = word_count.get(word,0) + 1\n",
    "sorted_featrues = sorted(word_count, key=word_count.get, reverse=True)        \n",
    "for word in sorted_featrues[:10]:\n",
    "    print(f\"{word} : {word_count[word]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acac31c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %conda install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174fe92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer 문서를 벡터화\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(vocabulary=sorted_features)\n",
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614d192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [movie_reviews.raw(fileid) for fileid in movie_revieews.fileids()]\n",
    "reviews[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
