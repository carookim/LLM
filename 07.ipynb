{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d06da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch09 01.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99c20fd",
   "metadata": {},
   "source": [
    "### 딥러닝 기반 감성분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806a5c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 딥러닝모델에 넣기위한 텍스트 전처리 방법 정리\n",
    "# 데이터 - 토큰화 & 정수 인코딩 - 시퀀스 패딩\n",
    "# baseline 일반 신경망 Dense\n",
    "# Simple RNN 기울기소실문제\n",
    "# Bidirectional LSTM 양방향 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3fde52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization 텍스트를 숫자로 변환\n",
    "# Word Embedding 단어를 고차원 벡터로 변환\n",
    "# Sequence Padding 길이가 다른 문장을 같은 크기로 고정\n",
    "# Simple RNN\n",
    "# LSTM            RNN 의 경사소실 문제 해결 - 장기기억 소실\n",
    "# Bidirrctional LSTM 양방향 콘텐츠 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6bd36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 : 수치로 변환하는 과정\n",
    "# 토크나이저 3단계\n",
    "    # 1. fit_on_text 가장 빈도가 높은 단어의 인덱스를 구축해서 딕셔너리\n",
    "    # 2. texts_to_sequence 각 문서를 점수 시퀀스 변환\n",
    "    # 3. pd_sequence 길이 정규화(같은 길이)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b225d45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝에서 워드 임베딩 레이어 :\n",
    "# 단어를 모델이 계산할 수 있는 숫자 형태(숫자 벡터)로 바꾸고,\n",
    "# 벡터의 길이를 일정하게 맞춘 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b122760",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_reviews = [\n",
    "    \"this movie is great and wonderful\",\n",
    "    \"bad movie with poor acting\",\n",
    "    \"great movie absolutely wonderful\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa73da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이토치 버젼\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c9fc226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 단어 분할 및 빈도 계산\n",
    "all_words = []\n",
    "for i in [review.split() for review in sample_reviews]:\n",
    "    all_words.extend(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85be753d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movie', 3), ('great', 2)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 빈도\n",
    "word_freq = Counter(all_words)\n",
    "word_freq.most_common(2) # 빈도수가 높은 단어 상위 2개 조회"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5fe412d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Tokenizer 구현\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, num_words = 10, oov_token = 'UNK'):\n",
    "        self.num_words = num_words   # 상위 몇 개 단어까지 사용할지\n",
    "        self.oov_token = oov_token   # 사전에 없는 단어를 위한 토큰\n",
    "        self.word_index = {oov_token: 1}  # OOV 토큰은 1번 인덱스로 시작\n",
    "        self.index_word = {1: oov_token}  # 인덱스로 단어도 조회 가능\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "        '''\n",
    "        문장을 단어 인덱스로 변환\n",
    "        '''\n",
    "        all_words = []\n",
    "        for i in [review.split() for review in sample_reviews]:\n",
    "            all_words.extend(i)\n",
    "        word_freq = Counter(all_words)\n",
    "        # 빈도 높은 순서로 인덱스 부여\n",
    "        # oov 토큰을 1로 설정\n",
    "        self.word_index[self.oov_token] =1\n",
    "        self.index_word[1]  = self.oov_token\n",
    "        idx = 2\n",
    "        for word, _ in word_freq.most_common(self.num_words-1):\n",
    "            self.word_index[word] = idx\n",
    "            self.index_word[idx] = word\n",
    "            idx += 1\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        '''\n",
    "        텍스트를 정수 시퀀스로 변환\n",
    "        '''\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            seq = []\n",
    "            for word in text.split():\n",
    "                # 단어가 vocabulart에 있으면 인덱스를 사용, 없으면 oov\n",
    "                word_index = self.word_index.get(word, 1)\n",
    "                seq.append(word_index)\n",
    "            sequences.append(seq)\n",
    "        return sequences\n",
    "    \n",
    "# Tokenizer 생성 및 학습\n",
    "tokenizer = SimpleTokenizer(num_words=10, oov_token='UUK')\n",
    "tokenizer.fit_on_texts(sample_reviews)\n",
    "# tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6ff824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트를 시퀀스로 변환\n",
    "sequences = tokenizer.texts_to_sequences(sample_reviews)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13be7c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41f54dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩 구현 - 문자열의 길이를 동일하게 맞춘다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
