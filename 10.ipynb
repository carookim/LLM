{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fc15d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch12 01.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f02335",
   "metadata": {},
   "source": [
    "### 가상환경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb845064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda create -n [이름] python=[버전]\n",
    "# conda activate [이름]\n",
    "\n",
    "# conda deactivate\n",
    "# conda remove-n [이름] --all\n",
    "\n",
    "# tensorflow 안전지원 파이썬 버젼 3.11 로 가상환경 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fef9264",
   "metadata": {},
   "source": [
    "### 딥러닝 감성분석\n",
    "- 입력 ~ 1D Convolution + poling 반복 단계 과정정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c1e28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 감성분석 - 딥러닝에 넣을 데이터 변환 과정 => 정수배열\n",
    "# 입력 -> 토큰화 및 시퀀스 변화 -> 패딩(고정길이화) -> 임베딩(단어->벡터화)\n",
    "# -> 1D Convolution + poling 반복 -> Flatten ->  Dense(은닉) -> 출력(softmax, 이진분류)\n",
    "# -> 학습(Adam + binary_CrossEntropy) -> 검증 / 테스트 평가 -> 시각화 \n",
    "\n",
    "# 시퀀스 단계에서 문장별로 숫자로 변환한 숫자 리스트의 길이를 맞추는 단계, 패딩\n",
    "# 임베딩 단계에서 단어별의 관계와 의미를 수치로 표시한다. 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50c9dff",
   "metadata": {},
   "source": [
    "- 말뭉치 로딩(nltk) 데이터 로딩\n",
    "- 토큰화(빈고 기반 인덱싱) 텍스트를 숫자로 변환\n",
    "- 시퀀스 패딩 고정길이 배치 구성\n",
    "- 임베딩 단어를 dense vector 표현학습\n",
    "- 임베딩 발전\n",
    "    - 한계 : 작은 데이터에서는 일반화 부족\n",
    "    - 발전 : 사전학습(Word2vec) , 문맥적 임베딩(Bert, GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e25d242",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90352354",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c2d2a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 인덱스 : {'UNK': 1, 'love': 2, 'i': 3, 'this': 4, 'film': 5, 'really': 6, 'movie': 7, 'hate': 8, 'boring': 9, 'great': 10}\n",
      "원본 시퀀스 : [[3, 6, 2, 4, 7], [3, 8, 4, 9, 5], [2, 2, 1, 5]]\n",
      "패딩결과 : [[3 6 2 4 7 0]\n",
      " [3 8 4 9 5 0]\n",
      " [2 2 1 5 0 0]], 사이즈 : (3, 6)\n"
     ]
    }
   ],
   "source": [
    "# 토큰화\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# sample data\n",
    "texts = [\n",
    "    'I really love this movie',\n",
    "    'I hate this boring film',\n",
    "    'love love great film'\n",
    "] # texts에 딕셔너리 형태도 들어가던데 그건 뭔지 ^\n",
    "\n",
    "# 토큰화 객체(최대 단어 10, oov 토큰 지정)\n",
    "tokenizer = Tokenizer(num_words=10, oov_token='UNK') # ^\n",
    "# num_words\n",
    "# 상위 10개의 단어만 사전에 포함한다.\n",
    "\n",
    "# oov_token\n",
    "# 모델이 학습할 때 보지 못한 단어를\n",
    "# 하나의 특별한 토큰으로 치환할때 무엇으로 치환할지 설정\n",
    "\n",
    "# num_words에 지정된 10의 값은 ovv_token까지 합해서 10개의 단어 사전을 생성한다.\n",
    "# 그래서 단어인덱스에서 조회한\n",
    "# 11번째 마지막 항목 'great': 10 은 실제로 단어사전에 해당되지않는다.\n",
    "\n",
    "tokenizer.fit_on_texts(texts)\n",
    "print(f'단어 인덱스 : {tokenizer.word_index}')\n",
    "\n",
    "# 시퀀스로 변환 ^\n",
    "seqs = tokenizer.texts_to_sequences(texts)\n",
    "print(f'원본 시퀀스 : {seqs}')\n",
    "\n",
    "# 패딩(최대길이를 6)\n",
    "padded = pad_sequences(seqs,maxlen=6,padding='post') # ^\n",
    "# padding='post'\n",
    "# padding=pre'\n",
    "print(f'패딩결과 : {padded}, 사이즈 : {padded.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8ca552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "임베딩 텐서 모양 : (3, 6, 4)\n",
      "첫 문장 첫 단어 벡터 : [ 0.04132915  0.04712292  0.04930978 -0.04584622]\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 : 임베딩 레이어\n",
    "import tensorflow as tf\n",
    "# 패딩된 시퀀스 padded\n",
    "vocab_size = 11 # unk 포함 단어인덱스 최대값 + 1\n",
    "embed_dim = 4 # 작은 차원\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size,output_dim=embed_dim,input_length=6)\n",
    "]) # ^\n",
    "embeddings = model.predict(padded)\n",
    "print(f'임베딩 텐서 모양 : {embeddings.shape}') # (3, 6, 4)\n",
    "print(f'첫 문장 첫 단어 벡터 : {embeddings[0,0,:]}')\n",
    "\n",
    "# output_dim = 4\n",
    "# 단어 하나가 [0.12, -0.53, 0.88, 0.03]처럼 4개의 실수로 표현\n",
    "\n",
    "# output_dim을 4차원으로 지정한이유"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a66794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : (1, 6, 4)\n",
      "출력 : (1, 4, 2)\n",
      "출력값 : [[[0.9618641  1.0261158 ]\n",
      "  [2.8295808  0.        ]\n",
      "  [2.2912824  0.        ]\n",
      "  [0.         0.72740257]]]\n"
     ]
    }
   ],
   "source": [
    "# 1D Convolution\n",
    "# 1D Convolution은 \"문장 속 특징을 찾는 탐지기\",\n",
    "# Pooling은 \"중요한 것만 남기는 압축기\",\n",
    "# 반복하면\n",
    "# \"단어 조합 → 문장 전체 특징\"까지 점점 높은 수준의 감정 특징을 뽑을 수 있다.\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# 임의 시퀀스(배치=1, 길이=6, 임베딩=4)\n",
    "x= np.random.randn(1,6,4).astype('float32')\n",
    "conv = tf.keras.layers.Conv1D(\n",
    "    filters = 2 # 2개의 패턴을 감지\n",
    "    ,kernel_size = 3 # 3-gram\n",
    "    ,activation = 'relu' # 활성화 함수 설정\n",
    ")\n",
    "y = conv(x)\n",
    "print(f'입력 : {x.shape}')\n",
    "print(f'출력 : {y.shape}')\n",
    "print(f'출력값 : {y.numpy()}')\n",
    "\n",
    "# 입력 : (1, 6, 4)\n",
    "# 출력 : (1, 4, 2)\n",
    "# kernel_size = 3 으로 6개의 값을 3개씩 차례로 겨치게 묶어서 출력[1]은 4개\n",
    "# filters = 2로 지정 했으므로 출력[2]은 2개\n",
    "\n",
    "# 근데 여기서 다른 걸로 사용해서 패턴을 구할수는 없는건지 ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222f19b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaxPooling\n",
    "pool = tf.keras.layers.MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ade7b8",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca971ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
