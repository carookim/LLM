{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fc15d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch12 01.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f02335",
   "metadata": {},
   "source": [
    "### 가상환경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb845064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda create -n [이름] python=[버전]\n",
    "# conda activate [이름]\n",
    "\n",
    "# conda deactivate\n",
    "# conda remove-n [이름] --all\n",
    "\n",
    "# tensorflow 안전지원 파이썬 버젼 3.11 로 가상환경 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fef9264",
   "metadata": {},
   "source": [
    "### 딥러닝 감성분석\n",
    "- 입력 ~ 1D Convolution + poling 반복 단계 과정정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c1e28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 감성분석 - 딥러닝에 넣을 데이터 변환 과정 => 정수배열\n",
    "\n",
    "# 입력 -> 토큰화 및 시퀀스 변화 -> 패딩(고정길이화) -> 임베딩(단어->벡터화)\n",
    "# -> 1D Convolution + poling 반복 -> Flatten ->  Dense(은닉) -> 출력(softmax, 이진분류)\n",
    "# -> 학습(Adam + binary_CrossEntropy) -> 검증 / 테스트 평가 -> 시각화 \n",
    "\n",
    "# 시퀀스 단계에서 문장별로 숫자로 변환한 숫자 리스트의 길이를 맞추는 단계, 패딩\n",
    "# 임베딩 단계에서 단어별의 관계와 의미를 수치로 표시한다. 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50c9dff",
   "metadata": {},
   "source": [
    "- 말뭉치 로딩(nltk) 데이터 로딩\n",
    "- 토큰화(빈고 기반 인덱싱) 텍스트를 숫자로 변환\n",
    "- 시퀀스 패딩 고정길이 배치 구성\n",
    "- 임베딩 단어를 dense vector 표현학습\n",
    "- 임베딩 발전\n",
    "    - 한계 : 작은 데이터에서는 일반화 부족\n",
    "    - 발전 : 사전학습(Word2vec) , 문맥적 임베딩(Bert, GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e25d242",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90352354",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c2d2a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 인덱스 : {'UNK': 1, 'love': 2, 'i': 3, 'this': 4, 'film': 5, 'really': 6, 'movie': 7, 'hate': 8, 'boring': 9, 'great': 10}\n",
      "원본 시퀀스 : [[3, 6, 2, 4, 7], [3, 8, 4, 9, 5], [2, 2, 1, 5]]\n",
      "패딩결과 : [[3 6 2 4 7 0]\n",
      " [3 8 4 9 5 0]\n",
      " [2 2 1 5 0 0]], 사이즈 : (3, 6)\n"
     ]
    }
   ],
   "source": [
    "# 토큰화\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# sample data\n",
    "texts = [\n",
    "    'I really love this movie',\n",
    "    'I hate this boring film',\n",
    "    'love love great film'\n",
    "] # texts에 딕셔너리 형태도 들어가던데 그건 뭔지 ^\n",
    "\n",
    "# 토큰화 객체(최대 단어 10, oov 토큰 지정)\n",
    "tokenizer = Tokenizer(num_words=10, oov_token='UNK') # ^\n",
    "# num_words\n",
    "# 상위 10개의 단어만 사전에 포함한다.\n",
    "\n",
    "# oov_token\n",
    "# 모델이 학습할 때 보지 못한 단어를\n",
    "# 하나의 특별한 토큰으로 치환할때 무엇으로 치환할지 설정\n",
    "\n",
    "# num_words에 지정된 10의 값은 ovv_token까지 합해서 10개의 단어 사전을 생성한다.\n",
    "# 그래서 단어인덱스에서 조회한\n",
    "# 11번째 마지막 항목 'great': 10 은 실제로 단어사전에 해당되지않는다.\n",
    "\n",
    "tokenizer.fit_on_texts(texts)\n",
    "print(f'단어 인덱스 : {tokenizer.word_index}')\n",
    "\n",
    "# 시퀀스로 변환 ^\n",
    "seqs = tokenizer.texts_to_sequences(texts)\n",
    "print(f'원본 시퀀스 : {seqs}')\n",
    "\n",
    "# 패딩(최대길이를 6)\n",
    "padded = pad_sequences(seqs,maxlen=6,padding='post') # ^\n",
    "# padding='post'\n",
    "# padding=pre'\n",
    "print(f'패딩결과 : {padded}, 사이즈 : {padded.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8ca552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "임베딩 텐서 모양 : (3, 6, 4)\n",
      "첫 문장 첫 단어 벡터 : [ 0.04132915  0.04712292  0.04930978 -0.04584622]\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 : 임베딩 레이어\n",
    "import tensorflow as tf\n",
    "# 패딩된 시퀀스 padded\n",
    "vocab_size = 11 # unk 포함 단어인덱스 최대값 + 1\n",
    "embed_dim = 4 # 작은 차원\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size,output_dim=embed_dim,input_length=6)\n",
    "]) # ^\n",
    "embeddings = model.predict(padded)\n",
    "print(f'임베딩 텐서 모양 : {embeddings.shape}') # (3, 6, 4)\n",
    "print(f'첫 문장 첫 단어 벡터 : {embeddings[0,0,:]}')\n",
    "\n",
    "# output_dim = 4\n",
    "# 단어 하나가 [0.12, -0.53, 0.88, 0.03]처럼 4개의 실수로 표현\n",
    "\n",
    "# output_dim 출력 차원수는 4차원으로 적절히 설정\n",
    "\n",
    "# 임베딩된 단어 벡터를 시각화 ^\n",
    "# 단어 간 의미적 유사도 분포 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a66794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : (1, 6, 4)\n",
      "출력 : (1, 4, 2)\n",
      "출력값 : [[[0.9618641  1.0261158 ]\n",
      "  [2.8295808  0.        ]\n",
      "  [2.2912824  0.        ]\n",
      "  [0.         0.72740257]]]\n"
     ]
    }
   ],
   "source": [
    "# 1D Convolution\n",
    "# 1D Convolution은 \"문장 속 특징을 찾는 탐지기\",\n",
    "# Pooling은 \"중요한 것만 남기는 압축기\",\n",
    "# 반복하면\n",
    "# \"단어 조합 → 문장 전체 특징\"까지 점점 높은 수준의 감정 특징을 뽑을 수 있다.\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# 임의 시퀀스(배치=1, 길이=6, 임베딩=4)\n",
    "x= np.random.randn(1,6,4).astype('float32')\n",
    "conv = tf.keras.layers.Conv1D(\n",
    "    filters = 2 # 2개의 패턴을 감지\n",
    "    ,kernel_size = 3 # 3-gram\n",
    "    ,activation = 'relu' # 활성화 함수 설정\n",
    ")\n",
    "# tf.keras.layers.Conv1D(2, 3, activation='relu')\n",
    "y = conv(x)\n",
    "print(f'입력 : {x.shape}')\n",
    "print(f'출력 : {y.shape}')\n",
    "print(f'출력값 : {y.numpy()}')\n",
    "\n",
    "# 입력 : (1, 6, 4)\n",
    "# 출력 : (1, 4, 2)\n",
    "# kernel_size = 3 으로 6개의 값을 3개씩 차례로 겨치게 묶어서 출력[1]은 4개\n",
    "# filters = 2로 지정 했으므로 출력[2]은 2개\n",
    "\n",
    "# 근데 여기서 다른 걸로 사용해서 패턴을 구할수는 없는건지 ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222f19b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaxPooling\n",
    "pool = tf.keras.layers.MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ade7b8",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca971ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata2\\miniconda3\\envs\\conda_venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - acc: 0.5000 - loss: 0.6922\n",
      "Epoch 2/15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - acc: 0.5000 - loss: 0.6913\n",
      "Epoch 3/15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - acc: 0.7500 - loss: 0.6906\n",
      "Epoch 4/15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - acc: 1.0000 - loss: 0.6897\n",
      "Epoch 5/15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - acc: 1.0000 - loss: 0.6887\n",
      "Epoch 6/15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - acc: 1.0000 - loss: 0.6877\n",
      "Epoch 7/15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - acc: 1.0000 - loss: 0.6869\n",
      "Epoch 8/15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - acc: 1.0000 - loss: 0.6860\n",
      "Epoch 9/15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - acc: 1.0000 - loss: 0.6853\n",
      "Epoch 10/15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - acc: 1.0000 - loss: 0.6845\n",
      "Epoch 11/15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - acc: 1.0000 - loss: 0.6836\n",
      "Epoch 12/15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - acc: 1.0000 - loss: 0.6828\n",
      "Epoch 13/15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - acc: 1.0000 - loss: 0.6819\n",
      "Epoch 14/15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - acc: 1.0000 - loss: 0.6810\n",
      "Epoch 15/15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - acc: 1.0000 - loss: 0.6801\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer # 단어를 단어사전, 단어를 숫자로 변경\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences # 길이 맞추기\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "texts = [\n",
    "    'I really love this movie', # 긍정\n",
    "    'I hate this boring film', # 부정\n",
    "    'great love film', # 긍정\n",
    "    'boring hate film' # 부정\n",
    "]\n",
    "\n",
    "labels = np.array([0,1,0,1]) # 긍정 : 0 , 부정 : 1\n",
    "\n",
    "# 토큰화\n",
    "tokenizer = Tokenizer(num_words=10,oov_token='UNK')\n",
    "tokenizer.fit_on_texts(texts) # 단어 사전 생성\n",
    "# 시퀀스화\n",
    "seqs = tokenizer.texts_to_sequences(texts)\n",
    "    # 단어사전을 기반으로 단어들을 숫자로 변경()\n",
    "# 길이 맞추기\n",
    "x = pad_sequences(seqs,maxlen=6,padding='post')\n",
    "\n",
    "# 임베딩\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim = 11, output_dim = 8, input_length = 6),\n",
    "    tf.keras.layers.Conv1D(16,3,activation='relu'), # ^\n",
    "    # tf.keras.layers.Conv1D(filters, kernel_size, activation=...)\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(8,activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    # 마지막 출력 설정값에 어울리는 활성화함수를 선정 ^ :\n",
    "        # 1개 : sigmoid\n",
    "        # 2개 : softmax\n",
    "    # ^\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc']) # ^\n",
    "# binary_crossentropy\n",
    "# binary_crossentropy는 정답이 0 또는 1인 이진분류에서 사용되는 손실함수\n",
    "# 모델이 예측한 확률이 정답(0 or 1)과 얼마나 차이나는지를 계산해 손실값으로 나타낸다.\n",
    "# 예측이 정답에 가까울수록 손실값은 작아지고,\n",
    "# 멀수록 커져서 모델이 더 정확해지도록 학습을 유도한다.\n",
    "# -> 옵티마이저는 손실값이 최소화, 작아지는 쪽으로 가중치를 조절한다.\n",
    "\n",
    "history = model.fit(x,labels,epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6124b568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "최종훈련 정확도 : 1.0\n",
      "[[0.48907918]\n",
      " [0.5078457 ]\n",
      " [0.49548474]\n",
      " [0.5051141 ]]\n",
      "라벨 : [0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(x)\n",
    "print(f\"최종훈련 정확도 : {history.history['acc'][-1]}\")\n",
    "print(f'{preds}')\n",
    "print(f'라벨 : {labels}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f6eb51",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b07d8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.11.3-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\playdata2\\miniconda3\\envs\\conda_venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "Downloading regex-2025.11.3-cp311-cp311-win_amd64.whl (277 kB)\n",
      "Downloading click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "\n",
      "   -------- ------------------------------- 1/5 [regex]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ------------------------ --------------- 3/5 [click]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   ---------------------------------------- 5/5 [nltk]\n",
      "\n",
      "Successfully installed click-8.3.0 joblib-1.5.2 nltk-3.9.2 regex-2025.11.3 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "142d1784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading mocie_reviews: Package 'mocie_reviews' not\n",
      "[nltk_data]     found in index\n"
     ]
    }
   ],
   "source": [
    "# nltk 데이터로드\n",
    "import nltk\n",
    "nltk.download('mocie_reviews')\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d862ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 재현성 시드 고정\n",
    "# 재현성을 위해 랜덤 시드를 고정한다.\n",
    "# 넘파이, 텐서플로우, 파이썬 기본 random 모듈의 시드를 모두 42로 설정\n",
    "import random\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcdb5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로딩\n",
    "ids = movie_reviews.fileids() # ^\n",
    "# .fileids()\n",
    "# NLTK 코퍼스(corpus) 안에 포함된 파일들의\n",
    "# ID(이름 목록) 을 가져오는 메서드입니다.\n",
    "\n",
    "# ID를 순환문으로 돌려, 전체 데이터의 행과, 카테고리를 추출한다.\n",
    "reviews = [movie_reviews.raw(id) for id in ids]\n",
    "categories = [movie_reviews.categories(id) for id in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da76cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터\n",
    "max_words = 10000 # 최대 단어수\n",
    "    # 이전버젼에선 max_words가 num_words\n",
    "maxlen = 500 # 문서길이\n",
    "embed_dim = 64 # 임베딩 차원\n",
    "batch_size = 256 # batch_size\n",
    "epochs = 15 # epoch\n",
    "\n",
    "# 토큰화 + 시퀀스 변화 + 패딩\n",
    "tokenizer = Tokenizer(num_words = max_words, oov_token='UNK')\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "seqs = tokenizer.texts_to_sequences(reviews)\n",
    "x = pad_sequences(seqs,maxlen=maxlen,padding='post') # ^\n",
    "    # 이전버젼에선 truncating이 padding \n",
    "\n",
    "# 체이닝 기법으로\n",
    "# tokenizer.fit_on_texts(reviews).texts_to_sequences(reviews) 이런식으로\n",
    "# 하려고했지만 이럴려면 그값으로 바로 반환하는 기능이있어야 한다. ^\n",
    "\n",
    "# 라벨인코딩\n",
    "label_map = {'pos':0,'neg':1}\n",
    "y = np.array([label_map.get(c) for c in categories])\n",
    "\n",
    "# train / test 분할\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y, stratify=y, random_state=42, test_size=0.2)\n",
    "\n",
    "# 모델구성\n",
    "\n",
    "# 컴파일\n",
    "# 콜백(선택)\n",
    "# 학습\n",
    "# 학습결과 시각화\n",
    "# 테스트 평가\n",
    "# 임의의 데이터로 예측"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
