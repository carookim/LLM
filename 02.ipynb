{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eedcd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장간에 유사한 문장 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82637bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1단계: 필요한 데이터 다운로드\n",
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 2단계: 데이터 탐색\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# 데이터셋 크기 파악\n",
    "print(f\"전체 영화 리뷰 수: {len(movie_reviews.fileids())}\")\n",
    "print(f\"카테고리: {movie_reviews.categories()}\")  # ['neg', 'pos']\n",
    "print(f\"부정 리뷰: {len(movie_reviews.fileids(categories='neg'))}개\")\n",
    "print(f\"긍정 리뷰: {len(movie_reviews.fileids(categories='pos'))}개\")\n",
    "\n",
    "# 3단계: 첫 번째 리뷰 살펴보기\n",
    "first_review_id = movie_reviews.fileids()[0]\n",
    "first_review = movie_reviews.raw(first_review_id)\n",
    "print(f\"\\n첫 번째 리뷰 ID: {first_review_id}\")\n",
    "print(f\"원문 일부:\\n{first_review[:200]}\")\n",
    "\n",
    "# 4단계: 토큰화 결과 확인\n",
    "sentences = movie_reviews.sents(first_review_id)  # 문장 단위 토큰화\n",
    "words = movie_reviews.words(first_review_id)      # 단어 단위 토큰화\n",
    "\n",
    "print(f\"\\n문장 토큰화 (첫 2개):\")\n",
    "for i, sent in enumerate(sentences[:2]):\n",
    "    print(f\"  {i+1}: {sent}\")\n",
    "\n",
    "print(f\"\\n단어 토큰화 (첫 20개): {words[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db052e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RegressTokenizer : 정규표현식으로 정확한 토큰화\n",
    "# stowords : 문법적 기능을 제거하고 단어에 집중\n",
    "# 상위 N개 단어 선택 : 메모리 효율성과 노이즈 제거의 균형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eeddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOW - 수동으로 벡터 생성\n",
    "# 1단계: 모든 문서를 단어 리스트로 변환\n",
    "documents = [list(movie_reviews.words(fileid)) \n",
    "             for fileid in movie_reviews.fileids()]\n",
    "\n",
    "print(f\"전체 문서 수: {len(documents)}\")\n",
    "print(f\"첫 문서의 단어 수: {len(documents[0])}\")\n",
    "print(f\"첫 문서의 첫 50개 단어:\\n{documents[0][:50]}\")\n",
    "\n",
    "# 2단계: 전체 단어 빈도 계산 (불용어 제외 전)\n",
    "word_count = {}\n",
    "for doc in documents:\n",
    "    for word in doc:\n",
    "        word_count[word] = word_count.get(word, 0) + 1\n",
    "\n",
    "# 상위 10개 빈도 단어 확인\n",
    "sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\n상위 10개 빈도 단어:\")\n",
    "for i, (word, count) in enumerate(sorted_words[:10], 1):\n",
    "    print(f\"  {i}. '{word}': {count}회\")\n",
    "\n",
    "# 3단계: 불용어 제거 후 처리\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 정규표현식으로 3글자 이상의 단어만 추출\n",
    "tokenizer = RegexpTokenizer(r\"[\\w']{3,}\")\n",
    "# 영어 불용어 로드\n",
    "english_stops = set(stopwords.words('english'))\n",
    "\n",
    "# 모든 리뷰를 토큰화하고 불용어 제거\n",
    "processed_documents = []\n",
    "for fileid in movie_reviews.fileids():\n",
    "    raw_text = movie_reviews.raw(fileid)\n",
    "    tokens = [token for token in tokenizer.tokenize(raw_text) \n",
    "              if token not in english_stops]\n",
    "    processed_documents.append(tokens)\n",
    "\n",
    "# 처리 후 단어 빈도 재계산\n",
    "word_count_processed = {}\n",
    "for doc in processed_documents:\n",
    "    for word in doc:\n",
    "        word_count_processed[word] = word_count_processed.get(word, 0) + 1\n",
    "\n",
    "sorted_processed = sorted(word_count_processed.items(), \n",
    "                         key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\n전체 서로 다른 단어 수: {len(sorted_processed)}\")\n",
    "print(\"\\n처리 후 상위 10개 단어:\")\n",
    "for i, (word, count) in enumerate(sorted_processed[:10], 1):\n",
    "    print(f\"  {i}. '{word}': {count}회\")\n",
    "\n",
    "# 4단계: 특성 선택 (상위 1000개 단어)\n",
    "word_features = [word for word, count in sorted_processed[:1000]]\n",
    "print(f\"\\n특성으로 선택된 단어 수: {len(word_features)}\")\n",
    "print(f\"특성 예시: {word_features[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e077627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document, word_features):\n",
    "    \"\"\"\n",
    "    문서를 특성 벡터로 변환\n",
    "    \n",
    "    Args:\n",
    "        document: 토큰화된 단어 리스트\n",
    "        word_features: 특성으로 사용할 단어 리스트\n",
    "    \n",
    "    Returns:\n",
    "        document의 각 특성에 대한 빈도 리스트\n",
    "    \"\"\"\n",
    "    # 문서 내 단어 빈도 계산\n",
    "    word_count = {}\n",
    "    for word in document:\n",
    "        word_count[word] = word_count.get(word, 0) + 1\n",
    "    \n",
    "    # 특성 벡터 생성\n",
    "    features = []\n",
    "    for word in word_features:\n",
    "        # 특성 단어가 문서에 없으면 0\n",
    "        features.append(word_count.get(word, 0))\n",
    "    \n",
    "    return features\n",
    "\n",
    "# 테스트 실행\n",
    "test_features = ['one', 'two', 'teen', 'couples', 'solo']\n",
    "test_doc = ['two', 'two', 'couples']\n",
    "result = document_features(test_doc, test_features)\n",
    "\n",
    "print(\"테스트 단어 리스트:\", test_features)\n",
    "print(\"테스트 문서:\", test_doc)\n",
    "print(\"결과 벡터:\", result)\n",
    "print(\"→ 'two'가 2번, 'couples'가 1번, 나머지는 0\")\n",
    "\n",
    "# 모든 문서에 대해 특성 벡터 생성\n",
    "feature_sets = [document_features(doc, word_features) \n",
    "                 for doc in processed_documents]\n",
    "\n",
    "print(f\"\\n생성된 특성 벡터 수: {len(feature_sets)}\")\n",
    "print(f\"각 벡터의 차원: {len(feature_sets[0])}\")\n",
    "print(f\"\\n첫 문서 벡터 (처음 20개):\")\n",
    "for i, (word, count) in enumerate(zip(word_features[:20], feature_sets[0][:20])):\n",
    "    print(f\"  '{word}': {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101a7d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer가 위절차를 수행한다.\n",
    "# CountVectorizer 사용방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240049e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_documents[0][:5] # 문장을 토큰화(3개의 연속된 문장, 불용어 제거)\n",
    "for doc in processed_documents:\n",
    "    english_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa898bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문서의 고정된 길이의 벡터로 변환(모든 문서가 같은 차원)\n",
    "# 기계학습 알고리즘의 입력형식으로 변환\n",
    "def document_features(document, word_features):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa96035",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c70d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"C:\\\\Users\\\\Playdata2\\\\Downloads\\\\daum_movie_review.csv\"\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c59978",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_review = df.review[1]\n",
    "sample_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c60909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc35e20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명사만 추출\n",
    "okt.nouns(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de0f30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "okt.pos(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f46910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명사 동사 형용사 Noun Verb Adjective\n",
    "[word for word, tag in okt.pos(sample_review) if tag in ['Noun','Verb','Adjective']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1300d54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커스텀 토크나이저 함수로 생성\n",
    "def custom_tokenizer(doc):\n",
    "    return [word for word, tag in okt.pos(doc) if tag in ['Noun','Verb','Adjective']]\n",
    "# 커스텀 토크나이저 함수를 만든 이유\n",
    "# CountVectorizer에 커스텀 토크나이저 적용하기위해서, 그니깐 간략하게 적을려고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68be93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer에 커스텀 토크나이저 적용\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "daum_cv = CountVectorizer(max_features=1000, tokenizer=custom_tokenizer)\n",
    "daum_dtm = daum_cv.fit_transform(df.review)\n",
    "daum_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623e7bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코사인 유사도 -1 ~ 1 사이의 값을 벡터간 각도기반 유사도 계산\n",
    "# 0 에 가까운 값 : 직교(무관) 1에 가까우면 같은 방향\n",
    "# 모든 유사도를 계산 : 상위 N개 결과 도출(추천 시스템의 기초) - 유튜브 알고리즘과 연관?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a188d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "reviews = df.review.copy()\n",
    "original_review = reviews[1]\n",
    "words = original_review.split()\n",
    "\n",
    "# 뒤 절반을 query 사용\n",
    "midpoint = len(words) // 2\n",
    "query_text = \" \".join(words[midpoint:])\n",
    "# 벡터로 변환\n",
    "query_vector = daum_cv.transform([query_text])\n",
    "query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6275996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184a50dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 리뷰와 유사도 계산\n",
    "# 첫번째 중간에서부터 뒷부분까지와 전체 데이터의 유사도를 비교하였다.\n",
    "import numpy as np\n",
    "similarity_scores = cosine_similarity(query_vector,daum_dtm)\n",
    "most_similar_idx = np.argmax(similarity_scores)\n",
    "\n",
    "# 가장 높은 유사도 값\n",
    "highest_score = similarity_scores[0, most_similar_idx]\n",
    "\n",
    "# 퍼센트로 변환\n",
    "similarity_percent = highest_score * 100\n",
    "\n",
    "print(f\"가장 유사한 리뷰 인덱스: {most_similar_idx}\")\n",
    "print(f\"유사도: {similarity_percent:.2f}%\")\n",
    "# 가장 유사한 리뷰  인덱스 가 첫번째 문장, 첫번째 문장 이 가장유사하다고 뜬....\n",
    "# 본인을 제외한 그 이외의 상위 5개의 문장을 조회해보고자한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe0a46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# query_vector: 쿼리 텍스트 벡터\n",
    "# daum_dtm: 전체 리뷰 DTM\n",
    "# daum_reviews: 전체 리뷰 리스트 또는 시리즈\n",
    "\n",
    "# 1. 모든 리뷰와 유사도 계산\n",
    "similarity_scores = cosine_similarity(query_vector, daum_dtm)[0]  # 1차원 배열\n",
    "\n",
    "# 2. 자기 자신(가장 높은 유사도) 제외\n",
    "most_similar_idx = np.argmax(similarity_scores)\n",
    "similarity_scores[most_similar_idx] = 0\n",
    "\n",
    "# 3. 상위 5개의 인덱스 추출\n",
    "top5_idx = np.argsort(similarity_scores)[-5:][::-1]  # 내림차순\n",
    "top5_scores = similarity_scores[top5_idx] * 100  # 퍼센트로 변환\n",
    "\n",
    "# 4. 출력\n",
    "print(\"원래 쿼리 텍스트:\")\n",
    "print(query_text)\n",
    "print(\"\\n본인을 제외한 상위 5개의 유사한 리뷰:\")\n",
    "\n",
    "for idx, score in zip(top5_idx, top5_scores):\n",
    "    print(f\"\\n리뷰 인덱스: {idx}, 유사도: {score:.2f}%\")\n",
    "    print(reviews[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0edf883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "url = \"https://drive.google.com/uc?id=1KOKgZ4qCg49bgj1QNTwk1Vd29soeB27o\"\n",
    "df = pd.read_csv(url)\n",
    "okt = Okt()\n",
    "def custom_tokenizer(doc):\n",
    "    \"\"\"\n",
    "    형태소 분석 후 명사, 동사, 형용사만 추출\n",
    "    \"\"\"\n",
    "    pos_tags = okt.pos(doc)\n",
    "    tokens = [word for word, pos in pos_tags \n",
    "              if pos in ['Noun', 'Verb', 'Adjective']]\n",
    "    return tokens\n",
    "\n",
    "daum_cv = CountVectorizer(\n",
    "    max_features=1000,\n",
    "    tokenizer=custom_tokenizer\n",
    ")\n",
    "\n",
    "reviews = df.review\n",
    "daum_dtm  = daum_cv.fit_transform(reviews)\n",
    "original_review = reviews[0]  # 첫 번째 리뷰\n",
    "print(f\"원본 리뷰 (처음 200자):\\n{original_review[:200]}\\n\")\n",
    "\n",
    "# 문서의 뒤 절반을 query로 사용 (부분 검색 시나리오)\n",
    "midpoint = len(original_review) // 2\n",
    "# query_text = original_review[midpoint:]  # 뒤 절반\n",
    "query_text = original_review\n",
    "print(f\"쿼리 텍스트 (처음 150자):\\n{query_text[:150]}\\n\")\n",
    "\n",
    "# 2단계: 쿼리 문서를 벡터로 변환\n",
    "query_vector = daum_cv.transform([query_text])\n",
    "print(f\"쿼리 벡터 크기: {query_vector.shape}\")\n",
    "\n",
    "# 유사도 분석\n",
    "scores = cosine_similarity(query_vector,daum_dtm)\n",
    "most_simular_idx = np.argmax(scores)\n",
    "# scores[most_simular_idx], reviews[most_simular_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54bcdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a39ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(reviews)[scores[0].argsort()[::-1][:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665227fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개선 TF-IDF\n",
    "# 단어의 상대적 중요도를 반영한 벡터화 기법\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,TfidfTransformer\n",
    "# TF : 특정단어가 문서에서 얼마나 자주나타나는 지 비율 (빈도)\n",
    "# 해당 단어의 빈도 / 문서의 전체 단어 수 좋다 문서에서 10 번 해당 문서는 100단어 10 / 100\n",
    "# IDF (Inverse Document Frequency) 단어가 전체 문장에서 얼마나 드문지 (희귀)\n",
    "# log(전체 문서 / 해당 단어 포함 문서) 단어의 가중치를 낮추기 위해서 log적용\n",
    "# 2000개 문서중 100개만 '좋다' log(2000/100) = 2.99\n",
    "# TF-IDF TF x IDF --> 특정 문서에서 의미 있는 단어에 높은 가중치를 부여한다.\n",
    "\n",
    "# 이 값이 클수록 문서내에서 단어 중요도가 높다 라고 판단하는 지표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28535d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_cv = TfidfVectorizer( # 사용법은 CounterVectorizer와 유사\n",
    "    max_features=1000,\n",
    "    tokenizer=custom_tokenizer\n",
    ")\n",
    "tfidf_dtm = tfidf_cv.fit_transform(reviews) # 전체 문서를 TF-IDF 벡터화\n",
    "\n",
    "count_dtm = daum_cv.fit_transform(reviews) # 전체 문서를 Bow 벡터화\n",
    "count_dtm.shape, tfidf_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b7d131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 쿼리 벡터화\n",
    "query_count = daum_cv.transform([query_text])\n",
    "query_tfidf = tfidf_cv.transform([query_text])\n",
    "\n",
    "# 코사인 유사도 계산\n",
    "count_sim = cosine_similarity(query_count,count_dtm)[0]\n",
    "tfidf_sim = cosine_similarity(query_tfidf,tfidf_dtm)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d6a987",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([1,2,3,4,5])\n",
    "a.argsort() # 오름차순으로 인덱스\n",
    "(-a).argsort() # 값에 -를 붙이면 .. 오름차순 인덱스는 내림차순과 같다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc589107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '돈 들인건 티가 나지만 보는 내내 하품만'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0489f3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer\t| TF-IDF\n",
    "# 단어 출현 횟수\t | 단어 출현 × 희귀 단어 가중치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77c0df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_count_index = (-count_sim).argsort()[:5]\n",
    "top5_tfidf_index = (-tfidf_sim).argsort()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0111cea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reviews = np.array(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23556f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[top5_count_index][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cf29f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 짧은 문장일수록 벡터의 크기가 작아,\n",
    "# Cosine similarity 계산에서 “작게나마 비슷하다”고 판단될 가능성이 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1323b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[top5_tfidf_index][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc05e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 의미 유사성”과 상관없이 벡터 차원에서 겹치는 단어가 있거나,\n",
    "# 희귀 단어 가중치 때문에 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eedb175",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_reivew = '숙면을 하기 좋은 영화.. 강추..'\n",
    "# my_review  count 방식이나 또는 tf-idf 방식으로 벡터화 한후... 전체리뷰를 만약 tf-idf 방식이면 전체 리뷰를 tf-idf 벡터화 한\n",
    "# 전체데이터와 함께 유사도 방식으로 점수를 구해서 상위 N개의 문서를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ba33c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 쿼리 벡터화\n",
    "query_count = daum_cv.transform([my_reivew])\n",
    "query_tfidf = tfidf_cv.transform([my_reivew ])\n",
    "\n",
    "# 코사인 유사도 계산\n",
    "count_sim = cosine_similarity(query_count,count_dtm)[0]\n",
    "tfidf_sim = cosine_similarity(query_tfidf,tfidf_dtm)[0]\n",
    "\n",
    "top5_count_index = (-count_sim).argsort()[:5]\n",
    "top5_tfidf_index = (-tfidf_sim).argsort()[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de351b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_reivew = '숙면을 하기 좋은 영화.. 강추..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589ff642",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reviews[top5_count_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d0d3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reviews[top5_tfidf_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c00aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram 연속된 N개의 단위\n",
    "# unigram 1-gram\n",
    "# 영화가 정말 재미있다\n",
    "# [영화] [가] [정말] [재미있다]\n",
    "\n",
    "# Bigram(2-gram)\n",
    "# [영화 가] [가 정말] [정말 재미있다]\n",
    "# 첫번째 단어와 마지막 단어는 한번씩 들어가겠네\n",
    "\n",
    "# Trigram(3-gram)\n",
    "# [영화 가 정말] [가 정말 재미있다]\n",
    "\n",
    "# 특성\n",
    "# 문맥정보 포함 : 단서의 순서와 관계를 반영\n",
    "# 더 나은 분류 : 좋은 영화 나쁜 영화 구분\n",
    "# 의미 보존 : 인접한 단어들의 의존성 보존\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a0e363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from konlpy.tag import Okt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3db4113",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedc6261",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rating.hist(bins=2) # 클래스 불균형 이 있어서 qcut을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01536ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['rating_group'] = pd.qcut(df['rating'], q=2, labels=[0, 1])\n",
    "\n",
    "# # 히스토그램\n",
    "# plt.hist(df['rating_group'], bins=2, edgecolor='black')\n",
    "# plt.xticks([0, 1])\n",
    "# plt.xlabel('Rating Group')\n",
    "# plt.ylabel('Count')\n",
    "# plt.show()\n",
    "## df.drop('rating_group', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0d667f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14723</th>\n",
       "      <td>간만에 제대로 잘짜여진 각본의 영화를 봤네 여운이 아직도 남아~어른을 위한 애니~</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.01.12</td>\n",
       "      <td>코코</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14724</th>\n",
       "      <td>한국개봉을 눈빠지게 기다린 보람이있다 깨우치는게 많은 영화</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.01.12</td>\n",
       "      <td>코코</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  rating        date  \\\n",
       "14723  간만에 제대로 잘짜여진 각본의 영화를 봤네 여운이 아직도 남아~어른을 위한 애니~      10  2018.01.12   \n",
       "14724               한국개봉을 눈빠지게 기다린 보람이있다 깨우치는게 많은 영화      10  2018.01.12   \n",
       "\n",
       "      title  label  \n",
       "14723    코코      1  \n",
       "14724    코코      1  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = (df.rating >= 7).astype(int)\n",
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0ff0c174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "원본 텍스트: '영화가 정말 재미있다'\n",
      "형태소 토큰: ['영화', '가', '정말', '재미있다']\n",
      "\n",
      "1-gram (Unigram):\n",
      "  총 4개: ['영화', '가', '정말', '재미있다']\n",
      "    1. [영화]\n",
      "    2. [가]\n",
      "    3. [정말]\n",
      "    4. [재미있다]\n",
      "\n",
      "2-gram (Bigram):\n",
      "  총 3개: ['영화 가', '가 정말', '정말 재미있다']\n",
      "    1. [영화 가]\n",
      "    2. [가 정말]\n",
      "    3. [정말 재미있다]\n",
      "\n",
      "3-gram (Trigram):\n",
      "  총 2개: ['영화 가 정말', '가 정말 재미있다']\n",
      "    1. [영화 가 정말]\n",
      "    2. [가 정말 재미있다]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def explain_ngrams(text, n_values=[1, 2, 3]):\n",
    "    \"\"\"N-gram을 이해하기 쉽게 설명\"\"\"\n",
    "    tokens = okt.morphs(text)\n",
    "    \n",
    "    print(f\"\\n원본 텍스트: '{text}'\")\n",
    "    print(f\"형태소 토큰: {tokens}\\n\")\n",
    "    \n",
    "    for n in n_values:\n",
    "        ngrams = [' '.join(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "        print(f\"{n}-gram ({['Unigram', 'Bigram', 'Trigram'][n-1]}):\")\n",
    "        print(f\"  총 {len(ngrams)}개: {ngrams}\")\n",
    "        if len(ngrams) <= 10:\n",
    "            for i, gram in enumerate(ngrams, 1):\n",
    "                print(f\"    {i}. [{gram}]\")\n",
    "        print()\n",
    "\n",
    "# 샘플 텍스트에 대한 N-gram 설명\n",
    "sample_review = \"영화가 정말 재미있다\"\n",
    "explain_ngrams(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b9aef9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "차원 : (14725, 50)\n",
      "!,,,.,..,...,?,cg,~,가,감동,것,고,과,그,너무,눈물,는,다,더,도\n"
     ]
    }
   ],
   "source": [
    "# 벡터화 n-gram별 벡터화 및 특성 비교\n",
    "def tokenizer_morphs(text):\n",
    "    '''\n",
    "    형태소 기반 토크나이저\n",
    "    '''\n",
    "    return okt.morphs(text)\n",
    "\n",
    "# 1-gram\n",
    "vec_1gram = TfidfVectorizer(tokenizer=tokenizer_morphs, ngram_range=(1,1), max_features=50)\n",
    "X_1gram = vec_1gram.fit_transform(df.review)\n",
    "print(f'차원 : {X_1gram.shape}')\n",
    "features_1gram = vec_1gram.get_feature_names_out()[:20]\n",
    "print(f'{','.join(features_1gram )}') # ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "865f28a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "차원 : (14725, 50)\n",
      "들 이\n"
     ]
    }
   ],
   "source": [
    "# 2-gram\n",
    "vec_2gram = TfidfVectorizer(tokenizer=tokenizer_morphs, ngram_range=(1,2), max_features=50)\n",
    "X_2gram = vec_2gram.fit_transform(df.review)\n",
    "print(f'차원 : {X_2gram.shape}')\n",
    "features_2gram = vec_2gram.get_feature_names_out()\n",
    "bigram = [f for f in features_2gram if len(f.split()) > 1 ][:15]\n",
    "print(f'{','.join(bigram)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e2ad272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(df.review,df.label,stratify=df.label,train_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8f545382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.799151103565365"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 gram 모델\n",
    "from sklearn.linear_model import LinearRegression\n",
    "vec_1 = TfidfVectorizer(tokenizer=custom_tokenizer,ngram_range=(1,1),max_features=1000)\n",
    "x_train_1 = vec_1.fit_transform(x_train)\n",
    "x_test_1 = vec_1.transform(x_test)\n",
    "\n",
    "clf_1 = LogisticRegression()\n",
    "clf_1.fit(x_train_1,y_train)\n",
    "clf_1.score(x_test_1,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "88609ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.38      0.51      3325\n",
      "           1       0.80      0.97      0.87      8455\n",
      "\n",
      "    accuracy                           0.80     11780\n",
      "   macro avg       0.80      0.67      0.69     11780\n",
      "weighted avg       0.80      0.80      0.77     11780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predict_1 = clf_1.predict(x_test_1)\n",
    "print( classification_report(y_test, predict_1) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
